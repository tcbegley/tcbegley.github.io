{"componentChunkName":"component---src-templates-post-js","path":"/blog/election-modelling-part-3","result":{"data":{"mdx":{"frontmatter":{"title":"Election Modelling - Part 3","date":"09 December 2019","path":"/blog/election-modelling-part-3","author":"Tom","excerpt":"This is the third and final post in a series on election modelling; specifically multi-level regression with poststratification (MRP) and its successful use by YouGov in the 2017 general election...","tags":["bayesian statistics","politics"],"coverImage":null},"id":"8fcf1a67-f12d-52e0-ba6b-0a532663d1d8","body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Election Modelling - Part 3\",\n  \"path\": \"/blog/election-modelling-part-3\",\n  \"date\": \"2019-12-09 08:00:00\",\n  \"author\": \"Tom\",\n  \"excerpt\": \"This is the third and final post in a series on election modelling; specifically multi-level regression with poststratification (MRP) and its successful use by YouGov in the 2017 general election...\",\n  \"tags\": [\"bayesian statistics\", \"politics\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"This is the third and final post in a series on election modelling; specifically\\nmulti-level regression with poststratification (MRP) and its successful use by\\nYouGov in the 2017 general election.\"), mdx(\"p\", null, \"In the previous posts we learnt the basics of conventional polling, Bayesian\\nstatistics, and computational inference. This should give us all the tools we\\nneed to understand how MRP works.\"), mdx(\"h2\", null, \"Overview\"), mdx(\"p\", null, \"Recall that in the first post we observed that one of the main issues with basic\\npolling analyses is that they are in some sense answering the wrong question.\\nSpecifically we are estimating national vote share, but often it can be hard to\\ntranslate this into a result, as support for the different parties is not evenly\\ndistributed.\"), mdx(\"p\", null, \"What we want to do is produce an estimate of vote share \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"in each constituency\"), \".\\nHowever, performing a separate poll in every constituency is impractical, both\\nfinancially and logistically. So we want to come up with a way to produce an\\nestimate in each constituency from a single national poll.\"), mdx(\"p\", null, \"We do this in two steps:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"We will build a model that predicts how an individual will vote based on\\ntheir demographics. The model will take into account things like age,\\neducation, past voting behaviour etc. This is the multilevel regression (MR)\\nof MRP.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Using the model, we can make predictions for each demographic group, then\\ncombine that with information about the demographics of each constituency to\\nproduce a constituency specific estimate of vote share. This is the\\npoststratification (P) of MRP.\")), mdx(\"p\", null, \"This way we can effectively use data collected at the national level to produce\\nlocal estimates. This high-level summary hides many interesting details, so lets\\nlook at each step in more detail.\"), mdx(\"h2\", null, \"Modelling the electorate\"), mdx(\"p\", null, \"Our goal is to build a model that can predict how someone will vote given their\\ndemographics. Let's start by considering a simple logistic regression model.\\nOnce we've done that we can use the simple model to understand\\npoststratification. Finally once we've understood the basics of both steps,\\nwe'll look at how we can build more powerful models.\"), mdx(\"p\", null, \"We'll assume the demographic features we have measured are age, sex, education,\\nvote at the last general election, and vote in the EU referendum. Writing out\\nthese models can get a bit messy, but I think it's generally helpful to see the\\nformulas. We'll use the following notation:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"inlineMath\"\n  }, \"P\"), \" is the number of parties we are including in the model.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"inlineMath\"\n  }, \"y_{i}\"), \" is the party of choice for individual \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"inlineMath\"\n  }, \"i\"), \". It is an integer in the\\nrange \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"inlineMath\"\n  }, \"1, \\\\dots, P\"), \".\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\gamma_{ip}\"), \" is the probability of individual \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"inlineMath\"\n  }, \"i\"), \" voting for party \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"inlineMath\"\n  }, \"p\"), \".\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"inlineMath\"\n  }, \"x^{age}_i\"), \" is the age of individual \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"inlineMath\"\n  }, \"i\"), \". We use similar notation for the\\nother features.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\alpha_p\"), \", and \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\beta^{age}_p\"), \" are parameters in the model, the index \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"inlineMath\"\n  }, \"p\"), \"\\ndetermining which party the prediction is made for.\")), mdx(\"p\", null, \"A simple choice of model would be logistic regression. Specifically we aim to\\nlearn parameters \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\alpha_p\"), \" and \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\beta^{age}_p\"), \", \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\beta^{sex}_p\"), \",\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\beta^{edu}_p\"), \", \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\beta^{ge}_p\"), \" and \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\beta^{eu}_p\"), \" for each party \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"p\"), \" such that\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\gamma_{ip}\"), \", the probability that individual \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"i\"), \" votes for party \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"p\"), \" satisfies\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"    \\\\gamma_{ip} = \\\\sigma(\\\\alpha_p + \\\\beta^{age}_p x^{age}_i + \\\\dots + \\\\beta^{eu}_p x^{age}_i),\"), mdx(\"p\", null, \"where \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\sigma\"), \" is the\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://en.wikipedia.org/wiki/Sigmoid_function\"\n  }, \"sigmoid function\"), \".\"), mdx(\"p\", null, \"For the non-ordered categorical variables such as \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"x^{ge}_i\"), \" we would really\\nneed to one-hot encode the feature and have multiple parameters, but I'm going\\nto ignore that for notational convenience.\"), mdx(\"p\", null, \"The party that individual \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"i\"), \" then selects as their preferred party is\\nunderstood as a categorical random variable with probabilities\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\boldsymbol{\\\\gamma}_{i}\")), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"    y_{i} \\\\sim \\\\mathrm{Categorical}(\\\\boldsymbol{\\\\gamma}_i).\"), mdx(\"p\", null, \"That's pretty much all we need for the simple version of this model! We can\\nsurvey a portion of the electorate, ask them not only who their first choice of\\nparty is, but also their demographics. Given this data we can learn the\\nparameters \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\boldsymbol{\\\\alpha}\"), \", \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\boldsymbol{\\\\beta}^{age}\"), \" etc. in the\\nstandard way. Then given a new individual, we can calculate their probabilities\\nof favouring each party, \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\boldsymbol{\\\\gamma}\"), \", and convert that into a choice\\nby sampling from the categorical distribution with those probabilities.\"), mdx(\"h2\", null, \"Poststratification\"), mdx(\"p\", null, \"Suppose we have a way of predicting how someone will vote given their\\ndemographics. It might be as simple as the logistic regression model we looked\\nat above, or it could be something much more complicated. Poststratification\\nallows us to use these predictions to estimate vote share in each constituency.\"), mdx(\"p\", null, \"The idea is actually pretty simple: to get the vote total for a particular\\nparty, we will sum up the number of people in the constituency of each\\ndemographic group, weighted by the probability that demographic group votes for\\nthe party in question.\"), mdx(\"p\", null, \"For example, let's suppose our model predicts that 25-34 year old women with a\\nuniversity education who voted Labour at the last election and voted to remain\\nin the EU have a probability \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"0.85\"), \" of voting Labour this time around. Then if\\nthere are 100 25-34 year old women with a university education who voted Labour\\nat the last election and voted to remain in the EU, we expect that Labour will\\nget 85 votes from this demographic group.\"), mdx(\"p\", null, \"We repeat that process for all demographic groups, multiplying the number of\\npeople in the group by the probability any one of them will vote for a\\nparticular party. Summing the totals over all groups we get an estimate for the\\nnumber of votes that party will get in that constituency.\"), mdx(\"p\", null, \"This gives us estimates that are specific to each constituency. There are two\\nproblems though. The first is that in order to carry out this procedure we need\\nto know how many representatives of each demographic group that we are modelling\\nlives in each constituency. Building such a picture of the demographic makeup\\ntypically requires the combination of a number of data sources and some\\nadditional modelling.\"), mdx(\"p\", null, \"The challenge is really that we need to estimate the full joint distribution of\\nthe demographics we are modelling, but often we just have access to marginal\\ndistributions. For example we know how many people voted for a given party in\\nany particular constituency, but we don't actually know who those people were.\"), mdx(\"p\", null, \"YouGov have not revealed much about how they built this demographic model of the\\nUK, but they did say they used a combination of census data, the annual\\npopulation survey and their own proprietary data from their panel.\"), mdx(\"p\", null, \"The second problem is that we have assumed that demographics affect an\\nindividual's party preference in the same way in every single constituency.\\nIdeally we want to train a model that produces probabilities for each\\ndemographic group specific to the constituency they live in. That's what we'll\\ndo next.\"), mdx(\"h2\", null, \"Adding constituency features\"), mdx(\"p\", null, \"The first thing we can do is to include constituency-level features in the model\\nin addition to the individual-level features. There are many options here,\\nthings like population density (as a proxy for urban / rural divides),\\nproportion of students, proportion who voted to leave the EU etc. tend to\\ncorrelate pretty well with outcomes.\"), mdx(\"p\", null, \"For simplicity let's suppose that we just include population density in our\\nmodel denoted \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"c^{pd}_j\"), \". Including additional features is completely analogous.\\nWe want to learn parameters \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\xi^{pd}_p\"), \" such that\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"    \\\\gamma_{ip} = \\\\sigma(\\\\alpha_p + \\\\xi^{pd}_p c^{pd}_{j(i)} + \\\\beta^{age}_p x^{age}_i + \\\\dots + \\\\beta^{eu}_p x^{age}_i).\"), mdx(\"p\", null, \"We use the notation \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"j(i)\"), \" to denote the constituency that individual \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"i\"), \" lives\\nin. The parameters \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\xi^{pd}_p\"), \" adjust the probabilities \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\gamma_{ip}\"), \" for the\\npopulation density of constituency \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"j(i)\"), \". This means our predictions now depend\\non the characteristics of the constituency, not just the demographics of the\\ninhabitants. This is an improvement, but we can still do better.\"), mdx(\"h2\", null, \"Hierarchical models\"), mdx(\"p\", null, \"One shortcoming of our model so far is that it implicitly assumes that the\\neffect of individual features on vote choice does not vary between\\nconstituencies. In reality we might expect this to vary. Perhaps in rural\\nconstituencies age does not correlate as strongly with likelihood of voting\\nLabour as it does in urban constituencies.\"), mdx(\"p\", null, \"We could try not pooling our data, learning different values for the parameters\\nin every constituency. In that case though, the small amounts of data in each\\nconstituency could lead to estimates with limited utility.\"), mdx(\"p\", null, \"Ideally we want to find an approach that represents a compromise between these\\ntwo extremes of pooling all our data, and modelling each constituency as\\ncompletely independent. A pragmatic approach might be to compute a weighted\\naverage of the pooled estimate and the unpooled estimate. Then the estimates for\\neach constituency are close to the national estimate, but also are allowed to\\nvary based on the data observed in that constituency. As a result we are less\\nlikely to make an extreme estimate due to small data, but we also get estimates\\ntailored to each constituency.\"), mdx(\"p\", null, \"It turns out that hierarchical models are a principled way of achieving this\\ncompromise, and in some cases are actually equivalent to taking a weighted\\naverage of the two estimates. Rather than explicitly taking an average, we\\nachieve the compromise through our choice of prior.\"), mdx(\"h3\", null, \"Priors as regularisers\"), mdx(\"p\", null, \"We saw in part 2 that when we have a lot of data, our inferences are not\\nterribly sensitive to our choice of prior. Only when we have small amounts of\\ndata is the posterior distribution of our parameters influenced significantly by\\nthe choice of prior distribution. This might be a cause for concern if you worry\\nabout the subjectivity of the prior.\"), mdx(\"p\", null, \"Conversely, suppose that the prior were well-informed, that is the knowledge it\\nrepresents is known to be accurate for some reason. Then we can view the prior\\nas capturing a large portion of our knowledge of the parameters, and the small\\namount of data we have is used to make fine adjustments.\"), mdx(\"p\", null, \"Let's think again about the coin flipping example. Suppose we are given a coin\\nand we need to estimate the probability of getting heads (denoted \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \") when\\nwe flip the coin. But suppose also that we only get to flip the coin three\\ntimes, we do and we get three heads. What should we estimate \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \" to be? If\\nwe took a non-Bayesian approach and calculated the\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://en.wikipedia.org/wiki/Maximum_likelihood_estimate\"\n  }, \"maximum likelihood estimate\"), \"\\nwe would estimate \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \" to be \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"1\"), \"! But such an estimate is probably\\nunreasonable...\"), mdx(\"p\", null, \"Say though that we knew something about the machine that made the coin. In\\nparticular we knew that the average value of \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \" for coins produced by the\\nmachine was \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"0.5\"), \", and the standard deviation in \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \" is \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"0.1\"), \". We could\\nencapsulate this knowledge by placing a \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\mathrm{Beta}(10, 10)\"), \" prior on\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \". Now if we observed three heads in a row, instead of estimating\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta = 1.0\"), \", we obtain a \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\mathrm{Beta}(13, 10)\"), \" posterior on \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \", which\\nhas mean \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"0.565\"), \". The full posterior looks like the below figure.\"), mdx(\"p\", {\n    \"align\": \"center\"\n  }, \"\\n  \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"558px\",\n      \"border\": \"8px solid white\",\n      \"borderRadius\": \"8px\",\n      \"background\": \"white\",\n      \"boxSizing\": \"content-box\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/48f1e2fdd1e94d77f96e5df219d1c93a/65207/low-data-posterior.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"82.5%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAIAAABSJhvpAAAACXBIWXMAAAsTAAALEwEAmpwYAAABmUlEQVQ4y5WTiW6DMAxA+f8/a7UycUwwlaMUaLmPhqsEzBwyqYXRSrMgMiaOnx1bmJ4EAKb/iDDOwt0Mw9jv96IoKopiWZau6x/ipyQrh8NBkiQ03u/3hXNd12ma8o+u6/JZiqKoCKnaNjcNEsdZXpRliUYMs3DGmG3broE4f9NM5hHOp4dlhU0pvd1um85w9aHI2VqRTX+haZo4jjerBZbB1uACWbrtvI2N0vfg2Exp21fkQt/3WIwN5iSGKGCftAfbfImdZdkCmzuHV0z41+Cdoa65usZG/7/Uo3mc8GL4Qb4LZfE3uIAJL7D577p6zhOrDa7z1pnHmduAXU8cPXYPA6s86vx5V+2uA2ReleDiQRKtgrP2jKJoShN6skYszMWjxvc4dwX2D+9HplAKJxv3DI4NTc1PYYMRBIGta22apJ7roNI0SZbbloUbMCPXdZEuy3PP82hZkjCMgyuPLuCLs0IqdhgdBjJfCVqQyHEcnKTdbofTpmmaqqqu78vqlyTLnEh4M66EEMwoDMNkFqTAKUBM1HlT/ABDRNpLsxpPCAAAAABJRU5ErkJggg==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"low data posterior\",\n    \"title\": \"low data posterior\",\n    \"src\": \"/static/48f1e2fdd1e94d77f96e5df219d1c93a/65207/low-data-posterior.png\",\n    \"srcSet\": [\"/static/48f1e2fdd1e94d77f96e5df219d1c93a/56d15/low-data-posterior.png 200w\", \"/static/48f1e2fdd1e94d77f96e5df219d1c93a/d9f49/low-data-posterior.png 400w\", \"/static/48f1e2fdd1e94d77f96e5df219d1c93a/65207/low-data-posterior.png 558w\"],\n    \"sizes\": \"(max-width: 558px) 100vw, 558px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n\"), mdx(\"p\", null, \"Here we see the regularising effect of the prior. Our knowledge of the situation\\npulls our estimates away from the extreme solutions suggested by the small\\namount of data we collected, and brings us back into the realm of something\\nreasonable. Rather than inferring that the coin is certain to come up heads, we\\ninstead infer that the coin is probably more likely to come up heads than not,\\nbut there is not enough data to fully overcome the prior.\"), mdx(\"p\", null, \"So if the prior can be used to regularise our estimates, the next question is\\nhow can we choose a prior? In general we won't have access to enough domain\\nknowledge to specify an informative prior like above. What we would like is a\\nprior that will regularise estimates of parameters in each constituency towards\\nthe estimates for the pooled data.\"), mdx(\"h3\", null, \"Enter the hyperprior\"), mdx(\"p\", null, \"The solution is to learn the prior itself from the data! Let's go back to the\\ncoin flipping example again. This time we assume that we have \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"J\"), \" coins\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"C_1, \\\\dots, C_J\"), \" with associated probabilities \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta_1, \\\\dots, \\\\theta_J\"), \" all\\nproduced by the same machine. We want to estimate each \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta_j\"), \", and like\\nbefore we have limited data, so we want to use a prior to regularise our\\nestimates. This time though, we aren't going to assume that we somehow know a\\ngood choice of prior.\"), mdx(\"p\", null, \"Since all the coins are assumed related, we use the same prior in each case\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\mathrm{Beta}(\\\\alpha, \\\\beta)\"), \". We don't know what values we should choose for\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\alpha\"), \" and \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\beta\"), \", so we place a prior on those too. This is known as a\\nhyperprior, as it is a prior on the parameters of our prior.\"), mdx(\"p\", null, \"Our full model might now look something like this\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"\\\\begin{array}{c}\\n    \\\\alpha, \\\\beta \\\\sim \\\\text{half-Cauchy}(0, 2.5)\\\\\\\\\\n    \\\\theta_j \\\\sim \\\\mathrm{Beta}(\\\\alpha, \\\\beta) \\\\hspace{20pt} j = 1, \\\\dots, J\\\\\\\\\\n    y_j \\\\sim \\\\mathrm{Binomial}(n_j, \\\\theta_j) \\\\hspace{20pt} j = 1, \\\\dots, J\\n\\\\end{array}\"), mdx(\"p\", null, \"Our sampling distribution is the same as it was before. Our prior has the same\\nform as before, and is shared by all the coins. This captures our knowledge that\\nthe coins were all produced by the same machine and hence are likely similar.\\nFinally we have added a hyperprior on \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\alpha\"), \" and \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\beta\"), \". Making a good choice\\nof hyperprior probably requires a bit more thought than we've given it here, in\\nthis case we just chose a relatively uninformative half-Cauchy distribution.\"), mdx(\"p\", null, \"We can get away with a less informative hyperprior, because the posterior for\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\alpha\"), \" and \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\beta\"), \" depend on all of the data, hence are less sensitive to the\\nsmall data problems. On the other hand the posterior for each \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta_j\"), \" depends\\nonly on data from coin \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"j\"), \".\"), mdx(\"p\", null, \"This still fits the paradigm of Bayesian inference, our parameters are\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\alpha, \\\\beta, \\\\theta_1, \\\\dots, \\\\theta_J\"), \", and we've broken up our prior using\\nthe chain rule\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"    p(\\\\alpha, \\\\beta, \\\\theta_1, \\\\dots, \\\\theta_J) = p(\\\\alpha, \\\\beta)p(\\\\theta_1, \\\\dots, \\\\theta_J | \\\\alpha, \\\\beta)\"), mdx(\"p\", null, \"Which is to say that mathematically nothing has really changed, but conceptually\\nwe've captured the hierarchical structure of the data in our model.\"), mdx(\"p\", null, \"So what is going to happen when we perform inference on this model? We only have\\na few observations of any individual coin, but we have many observations of coin\\nflips from coins produced by the machine. Collectively these inform the values\\nof \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\alpha\"), \" and \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\beta\"), \", which govern the prior on each \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta_j\"), \". Having\\nlearnt an informative prior for the \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta_j\"), \" from our data, we can effectively\\nregularise the small data estimates we would otherwise make for each coin.\"), mdx(\"h3\", null, \"Back to election models\"), mdx(\"p\", null, \"This is exactly what we now do with our election model. Rather than having a\\nsingle set of parameters \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\beta^{age}_p, \\\\dots, \\\\beta^{ge}_p\"), \" governing how an\\nindividuals demographics affect their vote choice, we have a separate set of\\nparameters for each constituency \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\beta^{age}_{jp}, \\\\dots, \\\\beta^{ge}_{jp}\"), \".\\nOrdinarily we wouldn't be able to effectively infer values for these parameters\\ndue to the low volumes of data in each constituency, however, by placing a\\nshared prior on these parameters, we learn reasonable values for the parameters\\nfrom all of the data, and then make constituency specific adjustments based on\\nthe data from that constituency. Now our model looks something like this\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"    \\\\gamma_{ip} = \\\\sigma(\\\\alpha_{j(i)p} + \\\\xi^{pd}_p c^{pd}_{j(i)} + \\\\beta^{age}_{j(i)p} x^{age}_i + \\\\dots + \\\\beta^{eu}_{j(i)p} x^{age}_i).\"), mdx(\"p\", null, \"I mentioned that we can think of priors as regularisers. In the case of\\nhierarchical models, the shared prior is like a regulariser that stops the\\nestimates for each group from diverging too much from each other by pulling the\\nestimates towards a pooled estimate. This is aligned with the fact that we\\nexpect the effect of demographics on vote choice to be roughly similar in each\\nconstituency.\"), mdx(\"p\", null, \"Another way to think of what's going on here is that you are sharing information\\neffectively between constituencies. While you ultimately are learning a separate\\nmodel in each constituency, the values of the parameters in each model are\\ninformed by the patterns being observed in all the other constituencies.\"), mdx(\"p\", null, \"With the model specified, we can use the computational inference techniques\\nspecified in the previous section to produce estimated vote totals in every\\nseat.\"), mdx(\"h2\", null, \"Closing thoughts\"), mdx(\"p\", null, \"In these three posts we took a lightning tour of polling, Bayesian statistics,\\ncomputational inference and hierarchical modelling. Safe to say that all of\\nthese topics are much larger than there is room to write about in three blog\\nposts, and in some places I've made simplifying assumptions or swept things\\nunder the carpet. Hopefully though these posts have given you a better idea of\\nhow these models are constructed. If you want to learn more,\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://benjaminlauderdale.net/files/papers/mrp-polling-paper.pdf\"\n  }, \"the paper\"), \"\\nwritten by the people that built the YouGov model is probably a good place to\\nstart.\"), mdx(\"p\", null, \"To wrap up I want to quickly mention one of the things that I glossed over:\\nmodelling turnout.\"), mdx(\"p\", null, \"In all of the above discussion we just thought about how you might predict an\\nindividuals first choice of political party. What we didn't consider is how\\nlikely that individual is to actually go out and vote for that party. This\\nrequires careful consideration, particularly because likelihood to turnout can\\nbe correlated with party preference. In the UK party preference is currently\\nstrongly correlated with age, and and so is turnout. As a result modelling\\nturnout badly can have a big impact on the final vote share estimates.\"), mdx(\"p\", null, \"For their 2017 forecasts, YouGov actually built two models. One to predict party\\npreference, one to predict turnout. These models were then combined for the\\npoststratification step, where the number of votes for each party from each\\ndemographic group depends not only on the probabilities that members of that\\ngroup will support each party, but also the probability that they will turn out\\nto vote in the first place.\"), mdx(\"p\", null, \"Their turnout model was another multilevel regression like the vote choice model\\ndescribed above. The main interesting thing about it is that it is trained on\\nvery different data. The vote choice model was trained on a large online survey\\nof voters, whereas the turnout model was trained on a small, face to face survey\\nconducted by the British Election study after the previous election. The reason\\nfor this is that turnout is regularly over-reported, people know that they ought\\nto vote, so they are likely to lie and say that they did vote when they didn't.\\nThough it means they have to use smaller sample sizes and older data, YouGov\\ngets a much more accurate picture of who is voting by using the face to face\\nsurveys. Their assumption is that likelihood of voting changes for an individual\\nmuch more slowly than their party preference, and so using the older data is not\\nso serious a problem.\"), mdx(\"p\", null, \"Anyway, as I write this we're in the final week of campaigning, a number of\\npollsters will likely be publishing their final predictions imminently. Whether\\nthey get it right again or whether there's an upset, hopefully you have more of\\nan insight now into how they got to their answer in the first place.\"));\n}\n;\nMDXContent.isMDXComponent = true;","excerpt":"This is the third and final post in a series on election modelling; specifically multi-level regression with poststratification (MRP) and its successful use by YouGov in the 2017 general election..."}},"pageContext":{"next":{"frontmatter":{"path":"/blog/bayesian-billiards","title":"Bayesian billiards","tags":["bayesian statistics","interactive"]},"fields":{"collection":"posts"},"fileAbsolutePath":"/home/runner/work/tcbegley.github.io/tcbegley.github.io/src/content/posts/bayesian-billiards.mdx"},"previous":{"frontmatter":{"path":"/blog/election-modelling-part-2","title":"Election Modelling - Part 2","tags":["bayesian statistics","politics","stan"]},"fields":{"collection":"posts"},"fileAbsolutePath":"/home/runner/work/tcbegley.github.io/tcbegley.github.io/src/content/posts/election-modelling-2.md"}}},"staticQueryHashes":["1425477374","3128451518"]}