{"componentChunkName":"component---src-templates-post-js","path":"/blog/bayesian-billiards","result":{"data":{"mdx":{"frontmatter":{"title":"Bayesian billiards","date":"06 December 2020","path":"/blog/bayesian-billiards","author":"Tom","excerpt":"In his seminal paper \"An Essay towards solving a Problem in the Doctrine of Chances\", Thomas Bayes introduced a thought experiment involving six balls thrown randomly onto a billiards table. ","tags":["bayesian statistics","interactive"],"coverImage":null},"id":"3705e2df-84f4-5240-9c88-998266a2f7f5","body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Bayesian billiards\",\n  \"path\": \"/blog/bayesian-billiards\",\n  \"date\": \"2020-12-06 08:00:00\",\n  \"author\": \"Tom\",\n  \"excerpt\": \"In his seminal paper \\\"An Essay towards solving a Problem in the Doctrine of Chances\\\", Thomas Bayes introduced a thought experiment involving six balls thrown randomly onto a billiards table. \",\n  \"tags\": [\"bayesian statistics\", \"interactive\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"In his seminal \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://royalsocietypublishing.org/doi/pdf/10.1098/rstl.1763.0053\"\n  }, \"paper\"), \" \\\"An Essay towards solving a Problem in the\\nDoctrine of Chances\\\", \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://en.wikipedia.org/wiki/Thomas_Bayes\"\n  }, \"Thomas Bayes\"), \" introduced a thought experiment\\ninvolving six balls thrown randomly onto a billiards table. This example is\\noften used in popular science as an introduction to the ideas of Bayesian\\nstatistics, as it nicely demonstrates the difference between a frequentist\\nmaximum likelihood estimate and a Bayesian point estimate. Having seen this\\nexample before, I had also heard a claim that the Bayesian estimate is \\\"slightly\\nbetter\\\", without having heard a clarification of what \\\"better\\\" means in this\\ncontext. In this post I'll describe the problem and compare the two approaches\\non simulated data.\"), mdx(\"h2\", null, \"The problem\"), mdx(\"p\", null, \"Let's begin by stating Bayes' thought experiment. This description is taken from\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://www.statslab.cam.ac.uk/~david/\"\n  }, \"The Art of Statistics\"), \" by David Spiegelhalter (which is a very\\nenjoyable read):\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Suppose a white ball is thrown at random on to a billiards table, its position\\nalong the table marked with a line, and then the white ball is removed. A\\nnumber of red balls are then thrown at random on to the table, and you are\\ntold only how many lie to the left and how many to the right of the line.\\nWhere do you think the line might be, and what should be your probability of\\nthe next red ball falling to the left of the line.\")), mdx(BilliardsContainer, {\n    mdxType: \"BilliardsContainer\"\n  }), mdx(\"p\", null, \"Let's set up some notation so that we can discuss possible solutions. First\\nwe'll denote the position of the white ball by \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \". The exact length of the\\ntable is irrelevant, so we'll assume that \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"0 \\\\leq \\\\theta \\\\leq 1\"), \", so that\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \" represents the proportion of the table to the left of the line. Since\\nthe ball is thrown randomly, it is reasonable to assume that\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"    \\\\theta \\\\sim \\\\mathrm{Unif}(0, 1)\"), mdx(\"p\", null, \"Next suppose that we have \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"n\"), \" red balls that we throw randomly onto the table.\\nThe probability that any one red ball stops to the left of the line is \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \",\\nso the number \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"y\"), \" of red balls that end up to the left of the line can be\\nmodelled as\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"    y \\\\sim \\\\mathrm{Binomial}(n, \\\\theta)\"), mdx(\"p\", null, \"Our goal is to come up with the best possible estimate of \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \" given \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"n\"), \" and\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"y\"), \".\"), mdx(\"h2\", null, \"The maximum likelihood estimate\"), mdx(\"p\", null, \"Maximum likelihood estimation produces an estimate for the unknown parameters in\\na model by choosing those parameters that maximise the likelihood. In our case,\\nwe want to maximise\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"    L(\\\\theta) := p(y | \\\\theta) = \\\\binom{n}{y}\\\\theta^y (1 - \\\\theta)^{n-y}\"), mdx(\"p\", null, \"Equivalently, since \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\log\"), \" is a monotone function, we can maximise the\\nlog-likelihood\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"    l(\\\\theta) := \\\\log(p(y | \\\\theta)) = y\\\\log\\\\theta + (n-y)\\\\log(1 - \\\\theta) + C\"), mdx(\"p\", null, \"where \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"C\"), \" is a constant independent of \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \". We can easily differentiate \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"l\"), \"\\nto find a local maximum at\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"    \\\\hat \\\\theta_{MLE} = \\\\frac{y}{n}\"), mdx(\"p\", null, \"In other words, the maximum likelihood estimate for the position of the line is\\nsimply the proportion of red balls that ended up to the left of the line.\"), mdx(\"h2\", null, \"A Bayesian point estimate\"), mdx(\"p\", null, \"We have already specified our prior (uniform distribution on \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \") and\\nsampling distribution (binomial distribution on \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"y\"), \"), so all that remains is to\\ncalculate the posterior. By Bayes' rule\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"    p(\\\\theta | y) \\\\propto p(\\\\theta)p(y | \\\\theta) \\\\propto \\\\theta^y (1 - \\\\theta)^{n-y}\"), mdx(\"p\", null, \"We recognise the right hand side as an unnormalised Beta density, and so\\nconclude that\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"    \\\\theta | y \\\\sim \\\\mathrm{Beta}(y + 1, n - y + 1)\"), mdx(\"p\", null, \"This gives us a distribution over possible positions of the line based on the\\nobserved data. In order to turn this into a point estimate, we could require for\\nexample that our expected squared loss over the posterior distribution is\\nminimised\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"    \\\\hat \\\\theta_{Bayes} = \\\\mathrm{argmin}_{\\\\tilde \\\\theta} \\\\mathbb{E}[(\\\\theta - \\\\tilde \\\\theta)^2 | y]\"), mdx(\"p\", null, \"which is in fact the same as simply choosing \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\hat \\\\theta_{Bayes}\"), \" to be the\\nposterior mean\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"    \\\\hat \\\\theta_{Bayes} = \\\\mathbb{E}[\\\\theta | y]\"), mdx(\"p\", null, \"In our case, with the Beta posterior this means that\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"    \\\\hat \\\\theta_{Bayes} = \\\\frac{y + 1}{n + 2}\"), mdx(\"p\", null, \"This is somewhat similar to the maximum likelihood estimate, but the modified\\nnumerator and denominator means that our estimates are always pulled back\\ntowards the center of the table. Most notable if \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"0\"), \" of the red balls landed to\\nthe left of the line, the maximum likelihood estimate is that the line is on the\\nleft edge of the table, whereas the Bayesian posterior mean puts the line at\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"1 / 7 \\\\approx 0.143\"), \", which intuitively seems more reasonable.\"), mdx(\"p\", null, \"This difference is a consequence of the fact that the Bayesian estimate has\\nbuilt into it the knowledge that we threw the white ball at random, and\\nmoderates the chance of making extreme inferences like drawing the line right on\\nthe edge of the table. This property is also more generally true of Bayesian\\nmethod: priors can help regularise our inferences, particularly when we have\\nlimited data.\"), mdx(\"h2\", null, \"Comparing the two estimates\"), mdx(\"p\", null, \"To compare the estimates, let's do some simulation. Using NumPy it's very easy\\nfor us to perform this experiment thousands or even millions of times.\"), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"python\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-python\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"import\"), \" numpy \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"as\"), \" np\\n\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"def\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token function\"\n  }, \"simulate\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"n_trials\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \"1_000_000\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" n_red\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"5\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \":\"), \"\\n    \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"# throw a white ball and n_red red balls onto n_trials tables\"), \"\\n    white \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \" np\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"random\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"random\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"size\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \"n_trials\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \"\\n    red \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \" np\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"random\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"random\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"size\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"n_trials\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" n_red\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \"\\n\\n    \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"# count the number of red balls to the left of the white ball\"), \"\\n    n_red_left \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"red \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"<\"), \" white\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"[\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \":\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token boolean\"\n  }, \"None\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"]\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token builtin\"\n  }, \"sum\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"axis\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"1\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \"\\n\\n    \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"# construct the two estimates\"), \"\\n    mle \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \" n_red_left \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"/\"), \" n_red\\n    posterior_mean \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"n_red_left \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"+\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"1\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"/\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"n_red \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"+\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"2\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \"\\n\\n    \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"return\"), \" white\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" mle\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" posterior_mean\"))), mdx(\"p\", null, \"We can repeat the experiment a million times, recording the true location of the\\nline, and the maximum likelihood and Bayesian estimates.\"), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"python\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-python\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"white\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" mle\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" posterior_mean \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \" simulate\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"1_000_000\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\")))), mdx(\"h3\", null, \"Brier score\"), mdx(\"p\", null, \"The Brier score is a \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://en.wikipedia.org/wiki/Scoring_rule#ProperScoringRules\"\n  }, \"proper scoring rule\"), \" that measures the\\nquality of probabilistic forecasts. Suppose we make a forecast\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"0 \\\\leq \\\\hat \\\\theta \\\\leq 1\"), \" that the next red ball will land to the left of the\\nline. We then observe the outcome \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"o = 0, 1\"), \". The Brier score for this single\\nforecast is\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"    (\\\\hat \\\\theta - o)^2\"), mdx(\"p\", null, \"that is, it's the square of the probability we assigned to the true outcome\\n\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"not\"), \" occuring. If we made a forecast \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\hat \\\\theta = 1\"), \", the Brier score would\\nbe \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"0\"), \" if we are correct and \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"o = 1\"), \", but \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"1\"), \" if we were wrong and \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"o = 0\"), \"! If\\nthe true outcome is not certain, maybe a more conservative forecast would be\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\hat \\\\theta = 0.7\"), \" in which case we would get a score of \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"0.3 ^ 2 = 0.09\"), \" if\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"o = 1\"), \" and \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"0.7^2 = 0.49\"), \" if \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"o = 0\"), \". In general, we minimise the expected\\nBrier score if \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\hat \\\\theta = P(o = 1)\"), \", meaning our forecast is encouraged to\\nbe well \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://en.wikipedia.org/wiki/Calibration_(statistics)#In_prediction_and_forecasting\"\n  }, \"calibrated\"), \".\"), mdx(\"p\", null, \"If we make a sequence \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\hat \\\\theta_i\"), \" of \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"N\"), \" forecasts with corresponding\\noutcomes, the Brier score is simply an average over all forecasts\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"    \\\\frac{1}{N} \\\\sum_{i=1}^N (\\\\hat \\\\theta_i - o_i)^2\"), mdx(\"p\", null, \"Why square the difference rather than take the absolute value? This is to\\nencourage proper calibration of forecasts. That is to say if an outcome occurs\\n70% of the time, the best forecast should be to assign a probability \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"0.7\"), \" to\\nthat outcome. If the score were the absolute difference \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"|\\\\hat \\\\theta - o|\"), \" then\\nwe are actually incentivised to exaggerate our confidence. If \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"P(o=1) = 0.7\"), \"\\nthen a forecast \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\hat \\\\theta \\\\equiv 0.7\"), \" yields a score\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"    \\\\mathbb{E}[|o - 0.7|] = 0.3 \\\\times 0.7 + 0.7 \\\\times 0.3 = 0.42\"), mdx(\"p\", null, \"whereas\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"    \\\\mathbb{E}[|o - 1|] = 0.3 \\\\times 1 + 0.7 \\\\times 0 = 0.3\"), mdx(\"p\", null, \"so our expected score is actually lower for the worse forecast!\"), mdx(\"h4\", null, \"Brier score - results\"), mdx(\"p\", null, \"I calculated the Brier scores for both estimates as follows\"), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"python\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-python\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"next_ball \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \" np\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"random\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"binomial\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"1\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" white\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \"\\n\\nmle_brier \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"mle \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"-\"), \" next_ball\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"**\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"2\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"mean\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \"\\nbayes_brier \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"posterior_mean \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"-\"), \" next_ball\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"**\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"2\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"mean\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \"\\n\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"print\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token string-interpolation\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token string\"\n  }, \"f\\\"MLE Brier score: \"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token interpolation\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token punctuation\"\n  }, \"{\"), \"mle_brier\", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token punctuation\"\n  }, \":\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token format-spec\"\n  }, \".4f\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token punctuation\"\n  }, \"}\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token string\"\n  }, \"\\\"\")), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \"\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"print\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token string-interpolation\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token string\"\n  }, \"f\\\"Bayes Brier score: \"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token interpolation\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token punctuation\"\n  }, \"{\"), \"bayes_brier\", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token punctuation\"\n  }, \":\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token format-spec\"\n  }, \".4f\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token punctuation\"\n  }, \"}\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token string\"\n  }, \"\\\"\")), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\")))), mdx(\"p\", null, \"which output\"), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"plaintext\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-plaintext\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-plaintext\"\n  }, \"MLE Brier score: 0.2002\\nBayes Brier score: 0.1906\"))), mdx(\"h2\", null, \"Conclusion\"), mdx(\"p\", null, \"While it did turn out that the Bayesian posterior mean wound up having a lower\\nBrier score, the difference was relatively small, so maybe it's not a compelling\\nreason to choose one approach over the other. It is however a nice illustration\\nof the fact that Bayesian inference is able to incorporate domain knowledge. In\\nthis case the fact that we know the white ball was thrown randomly and so would\\nbe equally likely to be anywhere on the table. The maximum likelihood estimate\\ndoesn't account for this knowledge, and is therefore prone to extreme inferences\\nlike estimating the line is right on one of the edges of the table.\"));\n}\n;\nMDXContent.isMDXComponent = true;","excerpt":"In his seminal paper \"An Essay towards solving a Problem in the Doctrine of Chances\", Thomas Bayes introduced a thought experiment involving six balls thrown randomly onto a billiards table. "}},"pageContext":{"next":{"frontmatter":{"path":"/blog/mcmc-part-1","title":"Implementing MCMC - the Metropolis algorithm","tags":["algorithms","bayesian statistics"]},"fields":{"collection":"posts"},"fileAbsolutePath":"/home/runner/work/tcbegley.github.io/tcbegley.github.io/src/content/posts/mcmc-part-1.md"},"previous":{"frontmatter":{"path":"/blog/election-modelling-part-3","title":"Election Modelling - Part 3","tags":["bayesian statistics","politics"]},"fields":{"collection":"posts"},"fileAbsolutePath":"/home/runner/work/tcbegley.github.io/tcbegley.github.io/src/content/posts/election-modelling-3.md"}}},"staticQueryHashes":["1425477374","3128451518"]}