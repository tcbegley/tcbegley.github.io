{"componentChunkName":"component---src-templates-post-js","path":"/blog/election-modelling-part-2","result":{"data":{"mdx":{"frontmatter":{"title":"Election Modelling - Part 2","date":"11 November 2019","path":"/blog/election-modelling-part-2","author":"Tom","excerpt":"This is the second post in a series on election modelling; specifically multi-level regression with poststratification (MRP) and its successful use by YouGov in the 2017 general election...","tags":["bayesian statistics","politics","stan"],"coverImage":null},"id":"9ba3d684-3a5e-5825-8f0e-8cc142c13a26","body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Election Modelling - Part 2\",\n  \"path\": \"/blog/election-modelling-part-2\",\n  \"date\": \"2019-11-11 08:00:00\",\n  \"author\": \"Tom\",\n  \"excerpt\": \"This is the second post in a series on election modelling; specifically multi-level regression with poststratification (MRP) and its successful use by YouGov in the 2017 general election...\",\n  \"tags\": [\"bayesian statistics\", \"politics\", \"stan\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"This is the second post in a series on election modelling; specifically\\nmulti-level regression with poststratification (MRP) and its successful use by\\nYouGov in the 2017 general election.\"), mdx(\"p\", null, \"In this post we are going cover technical preliminaries for understanding MRP.\\nIn particular we need to know a few things about Bayesian statistics, and\\ncomputational inference.\"), mdx(\"h2\", null, \"Bayesian statistics\"), mdx(\"p\", null, \"Bayesian statistics differs from the more common frequentist approach in a\\nsubtle way that has far reaching consequences. It comes down to a different\\ninterpretation of probabilities. Specifically Bayesians interpret probability as\\nquantifying the state of our knowledge, whereas frequentists interpret\\nprobability as frequencies of particular outcomes in repeated trials.\"), mdx(\"p\", null, \"Under a Bayesian interpretation of probability, it makes sense to make\\nprobabilistic statements about fixed, unknown quantities such as model\\nparameters; the probability quantifies our knowledge about the parameter.\"), mdx(\"p\", null, \"Under a frequentist interpretation on the other hand it does not make sense,\\nsince parameters are assumed fixed we can't repeat trials / experiments and\\ncalculate frequencies. These blog posts make no attempt to resolve the (at times\\npassionate) dispute between these two schools of thought, we're just interested\\nin learning the basics of the Bayesian approach.\"), mdx(\"p\", null, \"Because we are able to make probabilistic statements about model parameters when\\nwe have our Bayesian hats on, we can formulate and work with a joint probability\\nmodel for data and parameters. The recipe for doing this comes from Bayes'\\ntheorem. Let's denote by \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \" model parameters, and \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"y\"), \" data. Then\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"    p(\\\\theta | y) = \\\\frac{p(y|\\\\theta)p(\\\\theta)}{p(y)} \\\\propto p(y | \\\\theta) p(\\\\theta)\"), mdx(\"p\", null, \"Let's look at each term individually:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"inlineMath\"\n  }, \"p(\\\\theta \\\\vert y)\"), \" is called the \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"posterior distribution\"), \". It is the\\ndistribution of model parameters conditioned on the data, and as such\\nrepresents what we know about the model parameters having observed the data\\n\", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"inlineMath\"\n  }, \"y\"), \". This is what we want to calculate so that we may do inference.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"inlineMath\"\n  }, \"p(y \\\\vert \\\\theta)\"), \" is called the \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"sampling distribution\"), \" when viewed as a\\ndistribution over \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"inlineMath\"\n  }, \"y\"), \", or the \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"likelihood\"), \" when viewed as a function of\\n\", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \". It describes how data is generated given fixed model parameters.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"inlineMath\"\n  }, \"p(\\\\theta)\"), \" is called the \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"prior distribution\"), \". It is the distribution of\\nmodel parameters in the absence of any other information about \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"inlineMath\"\n  }, \"y\"), \". It\\nrepresents what we know about the model parameters before having observed any\\ndata.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"inlineMath\"\n  }, \"p(y)\"), \" is known as the \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"evidence\"), \" or \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"data marginal\"), \". We typically don't worry\\nabout it so much because it can be recovered by integrating the right hand\\nside with respect to \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \", and is just a normalising constant.\")), mdx(\"p\", null, \"The Bayesian workflow proceeds as follows: we specify a joint probability\\ndistribution on our data and parameters by specifying a prior and a sampling\\ndistribution. We then condition on the data to get the posterior, and use that\\nto make inferences about the parameters. We can then test our inferences and\\nrepeat the process to refine our model.\"), mdx(\"h2\", null, \"Example: coin flipping\"), mdx(\"p\", null, \"This can all be a little bit confusing if you've not seen it before, so it's\\nuseful to work through a concrete example. Let's imagine we have a coin, which\\nis not necessarily fair, and we want to infer the probability that the coin\\nshows heads when flipped. We'll collect data by repeatedly flipping the coin.\\nLet's denote by \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"n\"), \" the number of coin flips, \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"y\"), \" the the number of heads, and\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \" the probability of getting heads on a single flip.\"), mdx(\"p\", null, \"The first thing we need to do is specify a joint probability distribution over\\ndata and parameters via the prior and sampling distributions. The sampling\\ndistribution is relatively straightforward. Recall it describes how the outcome\\nof the coin flip \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"y\"), \" is generated given the parameter \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \". In this case \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"y\"), \"\\nshould count the number of \\\"successful\\\" outcomes (i.e. flips where we get heads)\\nin \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"n\"), \" independent flips where there is a fixed probability \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \" of\\n\\\"success\\\". This is the same thing as saying that \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"y\"), \" is Binomially distributed\\nwith \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"n\"), \" trials and probability \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \", that is to say our sampling\\ndistribution is \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"y \\\\vert \\\\theta \\\\sim \\\\mathrm{Binomial}(n, \\\\theta)\"), \".\"), mdx(\"p\", null, \"Next we need to specify a prior on \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \", which quantifies our knowledge\\nabout \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \" in the absence of any observed data. This comes a little less\\nnaturally, and many critics of Bayesian statistics say that the choice is\\nsubjective and biases inference with the statistician's own beliefs or\\npredjudices. We can view it as just another modelling assumption and ignore\\nthese concerns for now. Still, what is a reasonable choice? We can take a\\nmaximally unopinionated stance and say that without having flipped the coin, we\\nhave no way of knowing what the chances of getting heads is, and so we consider\\nany value of \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \" to be equally likely. Since \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \" is meant to represent\\na probability, it must be between \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"0\"), \" and \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"1\"), \", so we choose\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta \\\\sim \\\\mathrm{Uniform}(0, 1)\"), \" as our prior.\"), mdx(\"p\", null, \"To make inferences, we need to compute the posterior, which means multiplying\\nthe prior and sampling distributions together, and normalising. In this case\\nthough when multiplying the distributions we find\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"    p(\\\\theta | y) \\\\propto p(y | \\\\theta) p(\\\\theta) \\\\propto \\\\theta ^ y (1 - \\\\theta)^{n-y}\"), mdx(\"p\", null, \"We discard any constants independent of \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \" since they would be cancelled\\nin the normalisation anyway. On inspection we notice that the right hand side is\\nan unnormalised beta distribution in \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \", and so we deduce that\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta \\\\vert y \\\\sim \\\\mathrm{Beta}(y + 1, n - y + 1)\"), \".\"), mdx(\"p\", null, \"We'll look at how to use this distribution to make inferences in the next\\nsection, but first let's look at how the posterior changes as we observe coin\\nflips. To begin with, before we observe any data, the posterior is just the\\nprior.\"), mdx(\"p\", {\n    \"align\": \"center\"\n  }, \"\\n  \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"566px\",\n      \"border\": \"8px solid white\",\n      \"borderRadius\": \"8px\",\n      \"background\": \"white\",\n      \"boxSizing\": \"content-box\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/8deac3ab7ebe05193ebacf69c026bbf2/7ef45/beta11.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"77.5%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA4UlEQVQoz8VSQQ6DIBD0/0/zVOPJi1FrQeUgCAEriJ2K8dg2eOgGzTjZ3Zl1SbYLkfBxHBjbBPePu6eP74e03pijeFkWLoQ3elPTpuRPx9mjWAgxDEOk7UvFsD1Nk/c+ppgxRgiJVIamUmpd15jivu/jlfForSOVu66jlP5DGUtu29Y599wDmwvAWhs+AeZ5DswJwmoTICllWZZgsfCmaQDA1HUNgCtUVRVSOefIAUDOOebbNiiz33XoozcApggMTKVpWhRFnudZlqHjbY8wZvJ5KrTDH4EaeqEv3vACJth+Ab6SqlqJ6rYHAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"beta11\",\n    \"title\": \"beta11\",\n    \"src\": \"/static/8deac3ab7ebe05193ebacf69c026bbf2/7ef45/beta11.png\",\n    \"srcSet\": [\"/static/8deac3ab7ebe05193ebacf69c026bbf2/56d15/beta11.png 200w\", \"/static/8deac3ab7ebe05193ebacf69c026bbf2/d9f49/beta11.png 400w\", \"/static/8deac3ab7ebe05193ebacf69c026bbf2/7ef45/beta11.png 566w\"],\n    \"sizes\": \"(max-width: 566px) 100vw, 566px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n\"), mdx(\"p\", null, \"If we observed a tails, the posterior becomes \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\mathrm{Beta}(1, 2)\"), \" which looks\\nlike this.\"), mdx(\"p\", {\n    \"align\": \"center\"\n  }, \"\\n  \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"568px\",\n      \"border\": \"8px solid white\",\n      \"borderRadius\": \"8px\",\n      \"background\": \"white\",\n      \"boxSizing\": \"content-box\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/a0ac649e37e365c2f2fc3f4e0981803c/d63a8/beta12.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"77%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAAsTAAALEwEAmpwYAAABPUlEQVQoz5WS2XKDMAxF+f+PK20eyeKUhLDYrGHHXFeOIWGatKWayeQi60iyZEvdDICalVptFv183w+CQMUCbfsvXsPjOFZNg/KK/RZ5tp7XcN/3RVHoLynxyXDxJvivFBqO45hzfo+G4Dhs1YorWOavrmssq7UNDjsk8e+8htM0jaLo4ZuzwDvhfKKR/JTCMmWzLPt+Yq6QJTRFinjJP7X9xKu+A9uDR5NnEaZhz/P0nl/aHIrAh3tUclg6H5VHc7fFazMCczUU+bhzcL3eeYvOaMlJkjxjS36UUothkLQFERnvtGfzQpumockLIUhQOhJt25Ioy5L6EpxX1CCQeue+Hx5td11nrl1VFeUiQe2YFWw2G8dxwjC0bfvkuhT29v6R5/kE42Yv5yWlpMfHGHNdl7qjvCSOjFENOv0C7A1uwuc0i6gAAAAASUVORK5CYII=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"beta12\",\n    \"title\": \"beta12\",\n    \"src\": \"/static/a0ac649e37e365c2f2fc3f4e0981803c/d63a8/beta12.png\",\n    \"srcSet\": [\"/static/a0ac649e37e365c2f2fc3f4e0981803c/56d15/beta12.png 200w\", \"/static/a0ac649e37e365c2f2fc3f4e0981803c/d9f49/beta12.png 400w\", \"/static/a0ac649e37e365c2f2fc3f4e0981803c/d63a8/beta12.png 568w\"],\n    \"sizes\": \"(max-width: 568px) 100vw, 568px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n\"), mdx(\"p\", null, \"We can see the posterior probability density of \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta = 1\"), \" immediately goes to\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"0\"), \". This makes sense because we just observed tails, so we can rule out the\\npossibility that the coin is certain to come up heads. Similarly we are much\\nmore likely to have observed a single tails if \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \" is small and heads is\\nunlikely, which accounts for the decreasing slope. Let's suppose that we now\\nobserve two heads, the posterior becomes \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\mathrm{Beta}(3, 2)\"), \" which looks like\\nthis.\"), mdx(\"p\", {\n    \"align\": \"center\"\n  }, \"\\n  \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"583px\",\n      \"border\": \"8px solid white\",\n      \"borderRadius\": \"8px\",\n      \"background\": \"white\",\n      \"boxSizing\": \"content-box\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/081f6ddc219fb6675288a1b3fcb880fa/6d62d/beta32.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"74.5%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAAsTAAALEwEAmpwYAAABXElEQVQoz6VRy5aCMAz1/7/MlewFESkglEenyKMFmk6wCBad1eRQTpPm5t4kB/0PO+DhnGdZNk3T/hFg/vU9hDcIA5DiO/h+v4/j+InUzQN8D0Svu075nh6GPZgxFkXRF+ZhgMt5BcAPAxJudd9lW8xG8O0KnC+uiYRXLaXFXFWVxWzyqgJisvEswRLybHUX2XEcW7KVwlb1NO4bGUecnMVcliWldJFtGJIISd7bM5LmQwI9yA1cFAXKXnuGR43dfiBfdQuKn3EX5iRJFjAKvrjvU9mDaw5ZaoHTNB2eYAj8VTDYzKuLOZts3NMtDKe+U1dPlcX8jPyG51UCx2nu8yWJtmnjnivGAvfMc9oIERHSdh0GCSFCiDzPURqmoTpcKl5ySru2XcDGeF2Pc3lV17VSCkfQNA3GXdc9nU6+7zuO43leEATH4zGj1AL/ZVJKJERyLNq2LXuafE70F+7OaobXX9UXAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"beta32\",\n    \"title\": \"beta32\",\n    \"src\": \"/static/081f6ddc219fb6675288a1b3fcb880fa/6d62d/beta32.png\",\n    \"srcSet\": [\"/static/081f6ddc219fb6675288a1b3fcb880fa/56d15/beta32.png 200w\", \"/static/081f6ddc219fb6675288a1b3fcb880fa/d9f49/beta32.png 400w\", \"/static/081f6ddc219fb6675288a1b3fcb880fa/6d62d/beta32.png 583w\"],\n    \"sizes\": \"(max-width: 583px) 100vw, 583px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n\"), mdx(\"p\", null, \"Now that we've observed a heads, we can similarly rule out the possibility that\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta = 0\"), \", so the posterior density there also goes to zero. Furthermore,\\nwe've seen more heads than tails, so the posterior density is skewed towards\\nhigher values of \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \". Let's suppose now that we saw ten tails and twenty\\nheads, so the proportion of heads is the same, but the amount of data goes up.\\nNow the posterior is \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\mathrm{Beta}(21, 11)\"), \" which looks like this.\"), mdx(\"p\", {\n    \"align\": \"center\"\n  }, \"\\n  \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"553px\",\n      \"border\": \"8px solid white\",\n      \"borderRadius\": \"8px\",\n      \"background\": \"white\",\n      \"boxSizing\": \"content-box\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/5026119ae12277ba7b5b06594eef67e4/d35da/beta2111.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"78.5%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAAsTAAALEwEAmpwYAAABYklEQVQoz6VTf2+DIBT0+3+3Jsu6JTOdP1qtOCs6VLSKPHdIky4d9p+9qAG54473Ht7yj/CIaBzHJwgaeqr5OiIHWUq5wTNoSk/6/XVRyqHcNE1ZliuSHHylKDlSlpL4/ivuaa03lQGWHbEzdS2+DvL1eq2qyqFsPeeM6mrRsw4/HeRpmoQQ2+SMGmHWjpHjzEqptm23EkZxuGht00b94+k8HNhtGzHPFAe3fZA2q/ELZkrV9727SOWFvtjtB79g+kiG57quzU/Yw4J9rFWoNeIGRKtEq4t1aVvZ7t1LCg73KXAgq+nxzEVRLGWh4lAjK+dkPsUmPVEwd51eWaiIGdRcHfwZdobh3iSMsaPvD2i05JT4HwMvL3EUR9GoVMV5mqbAcc7Pea6kFFnGIWbJeNEnbddhME6qXbttnCZbvyAIdrtdGIZviP0+Y+xlb+JOfhJIBwzBGsqJXsJFyPPc3gXED5LbpqxYjE3oAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"beta2111\",\n    \"title\": \"beta2111\",\n    \"src\": \"/static/5026119ae12277ba7b5b06594eef67e4/d35da/beta2111.png\",\n    \"srcSet\": [\"/static/5026119ae12277ba7b5b06594eef67e4/56d15/beta2111.png 200w\", \"/static/5026119ae12277ba7b5b06594eef67e4/d9f49/beta2111.png 400w\", \"/static/5026119ae12277ba7b5b06594eef67e4/d35da/beta2111.png 553w\"],\n    \"sizes\": \"(max-width: 553px) 100vw, 553px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n\"), mdx(\"p\", null, \"Now the mode of the posterior distribution is closer to the proportion of heads\\nthat we've seen, and the variance has shrunk, i.e. we are more confident of the\\nlikely range of values for \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \".\"), mdx(\"p\", null, \"This illustrates a generic fact about the posterior distribution and its\\nrelationship with the prior, namely that as we observe more data, the influence\\nof the prior distribution diminishes, and the posterior distribution starts to\\nbecome more certain of the values of the parameters in the model.\"), mdx(\"p\", null, \"The following animation illustrates the coin flipping with two different choices\\nof prior. In this case the data was generated by simulating coin flips with a\\ncoin that had probability \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\frac 3 4\"), \" of coming up heads. We compare a uniform\\nprior and a \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\mathrm{Beta(5,11)}\"), \" prior that assumes the coin is much more\\nlikely to come up tails. To begin with after only a few coin flips the\\nposteriors differ significantly, but as we observe more data they get more and\\nmore similar as the influence of the prior fades.\"), mdx(\"undefined\", null, mdx(\"p\", {\n    \"align\": \"center\"\n  }, \"\\n  \"), mdx(\"div\", {\n    \"className\": \"gif-container\"\n  }, \"\\n    \", mdx(\"img\", {\n    parentName: \"div\",\n    \"src\": \"/56a8cf89da2d9191fcd686805ebf194c/animation.gif\",\n    \"width\": 900\n  }), \"\\n  \"), mdx(\"p\", null)), mdx(\"h2\", null, \"Inference with the posterior distribution\"), mdx(\"p\", null, \"Supposing we've specified the prior and computed the posterior, how do we\\nactually use it? This part turns out to be pretty natural. Most of the time we\\nwill want average some quantity of interest over the posterior distribution of\\nparameter values. For example, the posterior mean of the parameters is simply\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"  \\\\mathbb{E}(\\\\theta | y) = \\\\int \\\\theta p(\\\\theta | y) d\\\\theta.\"), mdx(\"p\", null, \"Similarly, if we wanted to calculate the probability of some new data \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\tilde y\"), \"\\ngiven observed data \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"y\"), \", we would calculate\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"  p(\\\\tilde y | y) = \\\\int p(\\\\tilde y | \\\\theta) p(\\\\theta | y) d\\\\theta.\"), mdx(\"p\", null, \"There is a natural interpretation of the right hand side in this context. We\\ncalculate the probability (density) of the new data \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\tilde y\"), \" for every\\npossible value of the parameters \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta\"), \", the \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"p(\\\\tilde y \\\\vert \\\\theta)\"), \" term;\\nthen we compute the weighted average of those where the terms involving more\\nlikely parameter values (according to the posterior) contribute more to the\\naverage.\"), mdx(\"p\", null, \"We can also calculate probabilities of parameters taking particular values.\\nSuppose in our coin flipping example we want to know whether\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta > \\\\frac 1 2\"), \". We can just calculate the probability directly by\\nintegrating\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"    P\\\\left(\\\\theta > \\\\frac 1 2 \\\\hspace{5pt} \\\\bigg | \\\\hspace{5pt} y \\\\right) = \\\\int_{\\\\frac 1 2}^1 p(\\\\theta | y) d\\\\theta\"), mdx(\"p\", null, \"The right hand side can be expressed in terms of the cumulative distribution\\nfunction (CDF) of the beta distribution. Since the beta distribution is a\\ncommonly used and well understood one-dimensional distribution, numerical\\nimplementations of the CDF are readily available. For example, let's suppose we\\nflip the coin thirty times, and observe twenty heads and ten tails. Based on the\\ncalculations we did above our posterior distribution is \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\mathrm{Beta}(21, 11)\"), \",\\nso in Python with SciPy we can do\"), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"python\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-python\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \">>\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \">\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"from\"), \" scipy\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"stats\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"distributions \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"import\"), \" beta\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \">>\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \">\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"1\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"-\"), \" beta\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"cdf\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"0.5\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"21\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"11\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \"\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"0.9646222270093858\")))), mdx(\"h2\", null, \"Inference with posterior samples\"), mdx(\"p\", null, \"In general, particularly with high-dimensional posterior distributions where we\\nhave many parameters in the model, approximating the integrals we need to\\nevaluate to do inference will be much harder. It turns out though, that we can\\nget good approximations to these integrals if we have access to samples from the\\nposterior distribution. For example, this code snippet draws ten thousands\\nsamples from the posterior, then calculates the proportion that are greater than\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"0.5\"), \". The answer is a close approximation to the answer obtained from the CDF\\nabove.\"), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"python\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-python\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \">>\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \">\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"import\"), \" numpy \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"as\"), \" np\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \">>\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \">\"), \" samples \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \" np\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"random\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"beta\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"21\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"11\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"10000\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \"\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \">>\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \">\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"samples \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \">\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"0.5\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"mean\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \"\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"0.9653\")))), mdx(\"p\", null, \"This is a specific example of using a \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://en.wikipedia.org/wiki/Monte_Carlo_integration\"\n  }, \"Monte Carlo estimate\"), \" to approximate\\nan integral. In general, if we have an integral of the form\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"  \\\\int f(\\\\theta) p(\\\\theta | y) d\\\\theta,\"), mdx(\"p\", null, \"we can approximate that with a sum of the form\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"  \\\\frac 1 S \\\\sum_{s=1}^S f(\\\\theta^s),\"), mdx(\"p\", null, \"where \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta^s \\\\hspace{5pt} s = 1, \\\\dots, S\"), \" are samples from the distribution\\ndefined by \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"p(\\\\theta \\\\vert y)\"), \". That is, we draw posterior samples then evaluate\\nthe integrand on those samples and average. If you are familiar with numerical\\nintegration methods, you can think of this like a grid approximation with a\\ndynamic grid, where the grid density is higher in regions where the probability\\ndensity is higher.\"), mdx(\"p\", null, \"Given a set of samples \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\theta^s \\\\hspace{5pt} s = 1, \\\\dots, S\"), \" and using the\\nintegral formulas above, we can approximate posterior means\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"  \\\\frac 1 S \\\\sum_{s=1}^S \\\\theta^s,\"), mdx(\"p\", null, \"and the posterior predictive distribution (the probability of observing new data\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"\\\\tilde y\"), \")\"), mdx(\"div\", {\n    \"className\": \"math\"\n  }, \"  \\\\frac 1 S \\\\sum_{s=1}^S p(\\\\tilde y | \\\\theta^s).\"), mdx(\"p\", null, \"In our case, with the samples we drew before, our approximation of the posterior\\nmean is\"), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"python\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-python\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \">>\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \">\"), \" samples\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"mean\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \"\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"0.6562683695235446\")))), mdx(\"p\", null, \"Similarly, if we wanted to predict the probability of getting a heads, then\\ntails (in that order) on the next two coin flips, we have\\n\", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"p(\\\\text{heads then tails} \\\\vert \\\\theta) = \\\\theta (1 - \\\\theta)\"), \", which we can\\napproximate with the samples as follows\"), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"python\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-python\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \">>\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \">\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"samples \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"*\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"1\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"-\"), \" samples\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"mean\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \"\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"0.2187\")))), mdx(\"p\", null, \"Just about any inference we might like to make about the parameters in our model\\ncan be calculated in a natural way like this, provided you can draw samples from\\nthe posterior distribution. Though it is out of scope for this particular series\\nof blog posts, a popular family of algorithms for drawing samples from arbitrary\\nprobability distirbutions are Markov chain Monte Carlo (MCMC) algorithms. One of\\nthe nice features of these algorithms is that they often don't require you to\\nnormalise the posterior distribution, it suffices to know it up to a\\nmultiplicative constant. In the context of Bayesian statistics where we\\ncalculate the posterior as the product of two terms that ordinarily would then\\nneed to be normalised, this is extremely convenient! If you are interested in\\nseeing how we could have used an MCMC sampler to sample from our posterior,\\ncheck out the addendum about drawing posterior samples using Stan.\"), mdx(\"p\", null, \"In the next post we'll describe hierarchical models of which multi-level\\nregression is an example.\"), mdx(\"h2\", null, \"Addendum: drawing posterior samples using Stan\"), mdx(\"p\", null, \"In the coin flipping example above, we knew the precise form of the posterior,\\nand it happened to be something that could easily be sampled from using\\nfunctions built into widely used libraries like NumPy. If we had a more\\ncomplicated posterior we would probably need to turn to specialised sampling\\ntools. In this addendum we'll take a quick look at how we can draw samples from\\nthe posterior using Stan and PyStan. We'll keep using the coin flipping example\\nso we can compare to earlier results, but everything we will do generalises to\\nmore complicated models and distributions.\"), mdx(\"p\", null, \"Stan implements a powerful sampling algorithm called Hamiltonian Monte Carlo\\n(HMC). The details of how it works are beyond the scope of this post, but there\\nis one important thing we should know in order to understand the output of Stan.\\nHMC, and MCMC algorithms more generally, draw correlated, not fully independent\\nsamples. Provided we properly account for the correlation in the samples though,\\nthis isn't a barrier to making inferences.\"), mdx(\"p\", null, \"Stan has its own modelling language which we use to specify the model. The Stan\\ncompiler \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"stanc\"), \" will transpile the Stan modelling language to optimized C++\\ncode, then compile the C++ code for us. The result is a very efficient sampling\\nalgorithm for the posterior distribution of our model. We won't discuss the\\nsyntax or features of the Stan modelling language here, check out the very good\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://mc-stan.org/users/documentation/\"\n  }, \"documentation\"), \" if you would like to know more.\"), mdx(\"p\", null, \"We need to define the data, parameters and then specify the prior and sampling\\ndistribution. We do so in three blocks of code labelled \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"data\"), \", \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"parameters\"), \" and\\n\", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"model\"), \". Stan does the rest by computing the posterior and drawing samples using\\nHMC. Here is the full model we are going to use.\"), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"stan\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-stan\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-stan\"\n  }, mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"data\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"{\"), \"\\n  \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"int\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token constraint\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token punctuation\"\n  }, \"<\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token property\"\n  }, \"lower\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token operator\"\n  }, \"=\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token expression\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token number\"\n  }, \"0\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token punctuation\"\n  }, \">\")), \" N\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \";\"), \"  \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"// number of coin flips\"), \"\\n  \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"int\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token constraint\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token punctuation\"\n  }, \"<\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token property\"\n  }, \"lower\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token operator\"\n  }, \"=\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token expression\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token number\"\n  }, \"0\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token property\"\n  }, \"upper\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token operator\"\n  }, \"=\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token expression\"\n  }, \"N\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token punctuation\"\n  }, \">\")), \" y\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \";\"), \"  \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"// number of heads\"), \"\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"}\"), \"\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"parameters\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"{\"), \"\\n  \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"real\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token constraint\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token punctuation\"\n  }, \"<\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token property\"\n  }, \"lower\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token operator\"\n  }, \"=\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token expression\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token number\"\n  }, \"0\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token property\"\n  }, \"upper\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token operator\"\n  }, \"=\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token expression\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token number\"\n  }, \"1\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token punctuation\"\n  }, \">\")), \" theta\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \";\"), \"  \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"// probability of heads\"), \"\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"}\"), \"\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"model\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"{\"), \"\\n  \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"// the prior\"), \"\\n  theta \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"~\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token function\"\n  }, \"uniform\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"0\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"1\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \";\"), \"\\n  \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"// the sampling distribution\"), \"\\n  y \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"~\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token function\"\n  }, \"binomial\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"N\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" theta\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \";\"), \"\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"}\")))), mdx(\"p\", null, \"Let's look at each block in turn. First the \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"data\"), \" block.\"), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"stan\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-stan\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-stan\"\n  }, mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"data\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"{\"), \"\\n  \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"int\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token constraint\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token punctuation\"\n  }, \"<\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token property\"\n  }, \"lower\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token operator\"\n  }, \"=\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token expression\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token number\"\n  }, \"0\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token punctuation\"\n  }, \">\")), \" N\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \";\"), \"  \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"// number of coin flips\"), \"\\n  \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"int\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token constraint\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token punctuation\"\n  }, \"<\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token property\"\n  }, \"lower\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token operator\"\n  }, \"=\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token expression\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token number\"\n  }, \"0\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token property\"\n  }, \"upper\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token operator\"\n  }, \"=\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token expression\"\n  }, \"N\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token punctuation\"\n  }, \">\")), \" y\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \";\"), \"  \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"// number of heads\"), \"\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"}\")))), mdx(\"p\", null, \"We specify that Stan should expect two pieces of information: the number of\\ntimes we flipped the coin \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"N\"), \", and the number of times it came up heads \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"y\"), \".\\nStan is a typed language, so we specify that both \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"N\"), \" and \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"y\"), \" are integers.\\nAdditionally, Stan allows us to place constraints on the data which will be\\nchecked at runtime. We specify that we can't have a negative number of coin\\nflips, and the number of heads should be non-negative and less than \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"N\"), \".\"), mdx(\"p\", null, \"Next the parameters block\"), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"stan\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-stan\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-stan\"\n  }, mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"parameters\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"{\"), \"\\n  \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"real\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token constraint\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token punctuation\"\n  }, \"<\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token property\"\n  }, \"lower\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token operator\"\n  }, \"=\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token expression\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token number\"\n  }, \"0\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token property\"\n  }, \"upper\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token operator\"\n  }, \"=\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token expression\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token number\"\n  }, \"1\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"token punctuation\"\n  }, \">\")), \" theta\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \";\"), \"  \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"// probability of heads\"), \"\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"}\")))), mdx(\"p\", null, \"We have only one parameter in our model, \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"theta\"), \". This is the probability that a\\nsingle flip will come up heads. As such it should be a real number between 0\\nand 1.\"), mdx(\"p\", null, \"Finally the model\"), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"stan\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-stan\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-stan\"\n  }, mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"model\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"{\"), \"\\n  \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"// the prior\"), \"\\n  theta \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"~\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token function\"\n  }, \"uniform\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"0\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"1\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \";\"), \"\\n  \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token comment\"\n  }, \"// the sampling distribution\"), \"\\n  y \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"~\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token function\"\n  }, \"binomial\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"N\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" theta\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \";\"), \"\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"}\")))), mdx(\"p\", null, \"We specify a uniform prior on theta (which is actually redundant, if we hadn't\\nspecified a prior on \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"theta\"), \" then Stan would have automatically chosen a uniform\\nprior for us), and a binomial sampling distribution for \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"y\"), \". The Stan syntax\\nmirrors mathematical notation quite nicely, so the model specification is quite\\nreadable.\"), mdx(\"p\", null, \"To compile the model and draw samples we're going to use the Python interface\\nPyStan. There are however many alternatives including interfaces for R, Matlab,\\nJulia and the command line. Details can be found in the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://mc-stan.org/users/documentation/\"\n  }, \"Stan\\ndocumentation\"), \". To get started with PyStan, install it with \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"pip\"), \".\"), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"sh\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-sh\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-sh\"\n  }, \"pip install pystan\"))), mdx(\"p\", null, \"Now we can load the model as a \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"pystan.StanModel\"), \" object and use the \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"sampling\"), \"\\nmethod to draw samples. Data is passed to the model using a Python dictionary as\\nfollows. We'll assume twenty heads and ten tails like we did previously.\"), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"python\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-python\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token keyword\"\n  }, \"import\"), \" pystan\\n\\nsm \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \" pystan\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"StanModel\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token builtin\"\n  }, \"file\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token string\"\n  }, \"\\\"model.stan\\\"\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \"\\nfit \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \" sm\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"sampling\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"data\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"{\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token string\"\n  }, \"\\\"N\\\"\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \":\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"30\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token string\"\n  }, \"\\\"y\\\"\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \":\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"20\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"}\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \",\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token builtin\"\n  }, \"iter\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"5000\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\")))), mdx(\"p\", null, \"If you run the above code snippet, Stan will create four chains (the default),\\nand draw 5000 samples from each. MCMC samplers can be sensitive to initial\\nconditions, so running multiple independent chains helps us assess whether the\\nchains have converged to the correct distributions. For each chain we discard\\nthe first 50% of samples as warmup, leaving us with a total of 10000 post-warmup\\ndraws from the four chains. Stan prints a summary of the sampling like the one\\nbelow.\"), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"text\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-text\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-text\"\n  }, \"Inference for Stan model: anon_model_bd7785d44829c52cfe28d5321faadbc2.\\n4 chains, each with iter=5000; warmup=2500; thin=1;\\npost-warmup draws per chain=2500, total post-warmup draws=10000.\\n\\n        mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\\ntheta   0.66  1.3e-3   0.08   0.49    0.6   0.66   0.71   0.81   3822    1.0\\nlp__  -21.09    0.01   0.69 -23.09 -21.25 -20.82 -20.64 -20.59   3845    1.0\\n\\nSamples were drawn using NUTS at Sun Nov 10 10:12:21 2019.\\nFor each parameter, n_eff is a crude measure of effective sample size,\\nand Rhat is the potential scale reduction factor on split chains (at\\nconvergence, Rhat=1).\"))), mdx(\"p\", null, \"For each parameter in our model, in this case \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"theta\"), \", we get an estimate of the\\nposterior mean, the standard error, the standard deviation and various\\npercentiles. This means at a glance we can read off some basic information about\\n\", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"theta\"), \" such as the posterior mean, which in this case is \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"0.66\"), \". We can also\\nsee that a central 95% credible interval for \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"theta\"), \" would be \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"0.49\"), \" to \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"0.81\"), \",\\nso there's still a fairly wide range of credible values.\"), mdx(\"p\", null, \"The final two columns \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"n_eff\"), \" and \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"Rhat\"), \" help us understand how correlated the\\nsamples are, and how well the sampler converged. \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"n_eff\"), \" counts the \\\"effective\\\"\\nnumber of samples, roughly speaking we can expect the correlated samples drawn\\nby Stan to have the utility of this number of independent samples. \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"Rhat\"), \"\\nmeasures how well the chains have converged and whether they have converged to\\nthe same thing. An \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"Rhat\"), \" value of \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"1.0\"), \" is necessary but not sufficient for\\nconvergence, i.e. if the chains have converged then \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"Rhat\"), \" will be \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"1.0\"), \", though\\nif \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"Rhat\"), \" is \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"1.0\"), \" convergence isn't guaranteed. In our case we have a large\\nnumber of effective samples and an \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"Rhat\"), \" of \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"1.0\"), \", so sampling appears to have\\nbeen successful.\"), mdx(\"p\", null, \"We can get the sampled values of \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"theta\"), \" and do computations with them manually,\\nfor example, let's calculate the probability that \", mdx(\"code\", {\n    parentName: \"p\",\n    \"className\": \"language-text\"\n  }, \"theta\"), \" is greater than \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"inlineMath\"\n  }, \"0.5\"), \"\\nand compare to our earlier answers\"), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"python\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-python\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \">>\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \">\"), \" samples \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"=\"), \" fit\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"extract\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"[\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token string\"\n  }, \"\\\"theta\\\"\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"]\"), \"\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \">>\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \">\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"samples \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \">\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"0.5\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"mean\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \"\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"0.9685\")))), mdx(\"p\", null, \"or the probability we next flip a heads, then tails\"), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"python\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-python\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \">>\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \">\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), \"samples \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"*\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"1\"), \" \", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token operator\"\n  }, \"-\"), \" samples\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \".\"), \"mean\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token punctuation\"\n  }, \")\"), \"\\n\", mdx(\"span\", {\n    parentName: \"code\",\n    \"className\": \"token number\"\n  }, \"0.21900253298714248\")))), mdx(\"p\", null, \"Both answers are in line with the results we saw earlier, but this time around\\nwe didn't have to explicitly compute the posterior distribution! Stan is able to\\ndo all of the heavy lifting for us. When dealing with more complicated\\ndistributions than what we've been considering, this turns out to be crucial for\\nmaking inference tractable.\"));\n}\n;\nMDXContent.isMDXComponent = true;","excerpt":"This is the second post in a series on election modelling; specifically multi-level regression with poststratification (MRP) and its successful use by YouGov in the 2017 general election..."}},"pageContext":{"next":{"frontmatter":{"path":"/blog/election-modelling-part-3","title":"Election Modelling - Part 3","tags":["bayesian statistics","politics"]},"fields":{"collection":"posts"},"fileAbsolutePath":"/home/runner/work/tcbegley.github.io/tcbegley.github.io/src/content/posts/election-modelling-3.md"},"previous":{"frontmatter":{"path":"/blog/election-modelling-part-1","title":"Election Modelling - Part 1","tags":["bayesian statistics","politics"]},"fields":{"collection":"posts"},"fileAbsolutePath":"/home/runner/work/tcbegley.github.io/tcbegley.github.io/src/content/posts/election-modelling-1.md"}}},"staticQueryHashes":["1425477374","3128451518"]}