{"componentChunkName":"component---src-templates-post-js","path":"/blog/election-modelling-part-2","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Election Modelling - Part 2","date":"11 November 2019","path":"/blog/election-modelling-part-2","author":"Tom","excerpt":"This is the second post in a series on election modelling; specifically multi-level regression with poststratification (MRP) and its successful use by YouGov in the 2017 general election...","tags":["bayesian statistics","politics","stan"],"coverImage":null},"id":"2551281b-989a-52af-9dd9-358e990a69a6","html":"<p>This is the second post in a series on election modelling; specifically\nmulti-level regression with poststratification (MRP) and its successful use by\nYouGov in the 2017 general election.</p>\n<p>In this post we are going cover technical preliminaries for understanding MRP.\nIn particular we need to know a few things about Bayesian statistics, and\ncomputational inference.</p>\n<h2>Bayesian statistics</h2>\n<p>Bayesian statistics differs from the more common frequentist approach in a\nsubtle way that has far reaching consequences. It comes down to a different\ninterpretation of probabilities. Specifically Bayesians interpret probability as\nquantifying the state of our knowledge, whereas frequentists interpret\nprobability as frequencies of particular outcomes in repeated trials.</p>\n<p>Under a Bayesian interpretation of probability, it makes sense to make\nprobabilistic statements about fixed, unknown quantities such as model\nparameters; the probability quantifies our knowledge about the parameter.</p>\n<p>Under a frequentist interpretation on the other hand it does not make sense,\nsince parameters are assumed fixed we can't repeat trials / experiments and\ncalculate frequencies. These blog posts make no attempt to resolve the (at times\npassionate) dispute between these two schools of thought, we're just interested\nin learning the basics of the Bayesian approach.</p>\n<p>Because we are able to make probabilistic statements about model parameters when\nwe have our Bayesian hats on, we can formulate and work with a joint probability\nmodel for data and parameters. The recipe for doing this comes from Bayes'\ntheorem. Let's denote by <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> model parameters, and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span> data. Then</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mi mathvariant=\"normal\">∣</mi><mi>θ</mi><mo stretchy=\"false\">)</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mfrac><mo>∝</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mi mathvariant=\"normal\">∣</mi><mi>θ</mi><mo stretchy=\"false\">)</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">    p(\\theta | y) = \\frac{p(y|\\theta)p(\\theta)}{p(y)} \\propto p(y | \\theta) p(\\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.363em;vertical-align:-0.936em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∝</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span></span>\n<p>Let's look at each term individually:</p>\n<ul>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta \\vert y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span> is called the <em>posterior distribution</em>. It is the\ndistribution of model parameters conditioned on the data, and as such\nrepresents what we know about the model parameters having observed the data\n<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>. This is what we want to calculate so that we may do inference.</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mi mathvariant=\"normal\">∣</mi><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(y \\vert \\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span> is called the <em>sampling distribution</em> when viewed as a\ndistribution over <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>, or the <em>likelihood</em> when viewed as a function of\n<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>. It describes how data is generated given fixed model parameters.</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span> is called the <em>prior distribution</em>. It is the distribution of\nmodel parameters in the absence of any other information about <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>. It\nrepresents what we know about the model parameters before having observed any\ndata.</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span> is known as the <em>evidence</em> or <em>data marginal</em>. We typically don't worry\nabout it so much because it can be recovered by integrating the right hand\nside with respect to <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>, and is just a normalising constant.</li>\n</ul>\n<p>The Bayesian workflow proceeds as follows: we specify a joint probability\ndistribution on our data and parameters by specifying a prior and a sampling\ndistribution. We then condition on the data to get the posterior, and use that\nto make inferences about the parameters. We can then test our inferences and\nrepeat the process to refine our model.</p>\n<h2>Example: coin flipping</h2>\n<p>This can all be a little bit confusing if you've not seen it before, so it's\nuseful to work through a concrete example. Let's imagine we have a coin, which\nis not necessarily fair, and we want to infer the probability that the coin\nshows heads when flipped. We'll collect data by repeatedly flipping the coin.\nLet's denote by <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">n</span></span></span></span> the number of coin flips, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span> the the number of heads, and\n<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> the probability of getting heads on a single flip.</p>\n<p>The first thing we need to do is specify a joint probability distribution over\ndata and parameters via the prior and sampling distributions. The sampling\ndistribution is relatively straightforward. Recall it describes how the outcome\nof the coin flip <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span> is generated given the parameter <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>. In this case <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>\nshould count the number of \"successful\" outcomes (i.e. flips where we get heads)\nin <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">n</span></span></span></span> independent flips where there is a fixed probability <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> of\n\"success\". This is the same thing as saying that <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span> is Binomially distributed\nwith <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">n</span></span></span></span> trials and probability <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>, that is to say our sampling\ndistribution is <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi><mi mathvariant=\"normal\">∣</mi><mi>θ</mi><mo>∼</mo><mrow><mi mathvariant=\"normal\">B</mi><mi mathvariant=\"normal\">i</mi><mi mathvariant=\"normal\">n</mi><mi mathvariant=\"normal\">o</mi><mi mathvariant=\"normal\">m</mi><mi mathvariant=\"normal\">i</mi><mi mathvariant=\"normal\">a</mi><mi mathvariant=\"normal\">l</mi></mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo separator=\"true\">,</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">y \\vert \\theta \\sim \\mathrm{Binomial}(n, \\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∼</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathrm\">B</span><span class=\"mord mathrm\">i</span><span class=\"mord mathrm\">n</span><span class=\"mord mathrm\">o</span><span class=\"mord mathrm\">m</span><span class=\"mord mathrm\">i</span><span class=\"mord mathrm\">a</span><span class=\"mord mathrm\">l</span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">n</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span>.</p>\n<p>Next we need to specify a prior on <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>, which quantifies our knowledge\nabout <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> in the absence of any observed data. This comes a little less\nnaturally, and many critics of Bayesian statistics say that the choice is\nsubjective and biases inference with the statistician's own beliefs or\npredjudices. We can view it as just another modelling assumption and ignore\nthese concerns for now. Still, what is a reasonable choice? We can take a\nmaximally unopinionated stance and say that without having flipped the coin, we\nhave no way of knowing what the chances of getting heads is, and so we consider\nany value of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> to be equally likely. Since <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> is meant to represent\na probability, it must be between <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span></span></span></span>, so we choose\n<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi><mo>∼</mo><mrow><mi mathvariant=\"normal\">U</mi><mi mathvariant=\"normal\">n</mi><mi mathvariant=\"normal\">i</mi><mi mathvariant=\"normal\">f</mi><mi mathvariant=\"normal\">o</mi><mi mathvariant=\"normal\">r</mi><mi mathvariant=\"normal\">m</mi></mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\theta \\sim \\mathrm{Uniform}(0, 1)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∼</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathrm\">U</span><span class=\"mord mathrm\">n</span><span class=\"mord mathrm\">i</span><span class=\"mord mathrm\" style=\"margin-right:0.07778em;\">f</span><span class=\"mord mathrm\">o</span><span class=\"mord mathrm\">r</span><span class=\"mord mathrm\">m</span></span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">1</span><span class=\"mclose\">)</span></span></span></span> as our prior.</p>\n<p>To make inferences, we need to compute the posterior, which means multiplying\nthe prior and sampling distributions together, and normalising. In this case\nthough when multiplying the distributions we find</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo stretchy=\"false\">)</mo><mo>∝</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mi mathvariant=\"normal\">∣</mi><mi>θ</mi><mo stretchy=\"false\">)</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mo stretchy=\"false\">)</mo><mo>∝</mo><msup><mi>θ</mi><mi>y</mi></msup><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mi>θ</mi><msup><mo stretchy=\"false\">)</mo><mrow><mi>n</mi><mo>−</mo><mi>y</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">    p(\\theta | y) \\propto p(y | \\theta) p(\\theta) \\propto \\theta ^ y (1 - \\theta)^{n-y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∝</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∝</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.714392em;\"><span style=\"top:-3.1130000000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">y</span></span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.071331em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.821331em;\"><span style=\"top:-3.1130000000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span><span class=\"mbin mtight\">−</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">y</span></span></span></span></span></span></span></span></span></span></span></span></span>\n<p>We discard any constants independent of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> since they would be cancelled\nin the normalisation anyway. On inspection we notice that the right hand side is\nan unnormalised beta distribution in <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>, and so we deduce that\n<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo>∼</mo><mrow><mi mathvariant=\"normal\">B</mi><mi mathvariant=\"normal\">e</mi><mi mathvariant=\"normal\">t</mi><mi mathvariant=\"normal\">a</mi></mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo>+</mo><mn>1</mn><mo separator=\"true\">,</mo><mi>n</mi><mo>−</mo><mi>y</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\theta \\vert y \\sim \\mathrm{Beta}(y + 1, n - y + 1)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∼</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathrm\">B</span><span class=\"mord mathrm\">e</span><span class=\"mord mathrm\">t</span><span class=\"mord mathrm\">a</span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8388800000000001em;vertical-align:-0.19444em;\"></span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">n</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7777700000000001em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">1</span><span class=\"mclose\">)</span></span></span></span>.</p>\n<p>We'll look at how to use this distribution to make inferences in the next\nsection, but first let's look at how the posterior changes as we observe coin\nflips. To begin with, before we observe any data, the posterior is just the\nprior.</p>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 566px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/8deac3ab7ebe05193ebacf69c026bbf2/7ef45/beta11.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 77.33333333333333%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA10lEQVQoz6WSzQ6DIBCEff+H82S0Hjy0pUkR0IgKiAodpWnTQ39CN0SH1c8ddk38H5E459q2lV3nGHXk5C7nL4scveBP2BjT9/1qtP9pKW+nl8pN00DE2B6GoYPtaFhKGQnDM5xHwtbarWHrGgMLITjnkXNGTaVUZGXGGI4dYLfHOxGYh9jgZVniK1/3wKi11hgbmod/DsODhsB1HMdpmpAPAhnUu8O4YU8IgYD/uq4hKKVoJESe53iEF4qiQBJDLcsSX9ngz8bmea6qKsuywx4wCJ2maYBvPUpvnWqZu60AAAAASUVORK5CYII=&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"beta11\" title=\"beta11\" src=\"/static/8deac3ab7ebe05193ebacf69c026bbf2/7ef45/beta11.png\" srcset=\"/static/8deac3ab7ebe05193ebacf69c026bbf2/a9205/beta11.png 150w,\n/static/8deac3ab7ebe05193ebacf69c026bbf2/a8a0d/beta11.png 300w,\n/static/8deac3ab7ebe05193ebacf69c026bbf2/7ef45/beta11.png 566w\" sizes=\"(max-width: 566px) 100vw, 566px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n  </a>\n    </span>\n</p>\n<p>If we observed a tails, the posterior becomes <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mrow><mi mathvariant=\"normal\">B</mi><mi mathvariant=\"normal\">e</mi><mi mathvariant=\"normal\">t</mi><mi mathvariant=\"normal\">a</mi></mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo separator=\"true\">,</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathrm{Beta}(1, 2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathrm\">B</span><span class=\"mord mathrm\">e</span><span class=\"mord mathrm\">t</span><span class=\"mord mathrm\">a</span></span><span class=\"mopen\">(</span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">2</span><span class=\"mclose\">)</span></span></span></span> which looks\nlike this.</p>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 568px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/a0ac649e37e365c2f2fc3f4e0981803c/d63a8/beta12.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 76.66666666666666%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAAsTAAALEwEAmpwYAAABNUlEQVQoz5WSC4+CMBCE+f9/z1xUpOKhBAjP8hCxdHrToCd3np42IWza/XZmu3XMfAHmneXwK4oijmMjK5xOb5Ww8DAMsq41YbFB27zOO9Ov6zpN4HzGTiCJX+zCwmVZRlH0nY0kwm4Lpf7lL8p932utb3zbQLioyue8M3lO0/T3nSuFYIfwYKAflbBw27Y/4FkJZCl8zzyYwp3t+7Gfemw9FPllZ1bCwhxyGIZ/tzWlAvSP/afR49zCRfl4PI7jeM3HPMBVDWWhxcZ07d0jkRJPp4KpqfOgtxtexFTbwlmW0Xae59RvmoZjZ1DXNQPeBXc4DqWUrKrWGtTNIaDezTbPgiCgODH7zo0hnCQJA9d1Pc+jtcViwZxayo/Viko3+KFbgI9vvV6zhO/7jIUQy+WSpnj6BWKmbtPEkQ01AAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"beta12\" title=\"beta12\" src=\"/static/a0ac649e37e365c2f2fc3f4e0981803c/d63a8/beta12.png\" srcset=\"/static/a0ac649e37e365c2f2fc3f4e0981803c/a9205/beta12.png 150w,\n/static/a0ac649e37e365c2f2fc3f4e0981803c/a8a0d/beta12.png 300w,\n/static/a0ac649e37e365c2f2fc3f4e0981803c/d63a8/beta12.png 568w\" sizes=\"(max-width: 568px) 100vw, 568px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n  </a>\n    </span>\n</p>\n<p>We can see the posterior probability density of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\theta = 1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span></span></span></span> immediately goes to\n<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span></span></span></span>. This makes sense because we just observed tails, so we can rule out the\npossibility that the coin is certain to come up heads. Similarly we are much\nmore likely to have observed a single tails if <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> is small and heads is\nunlikely, which accounts for the decreasing slope. Let's suppose that we now\nobserve two heads, the posterior becomes <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mrow><mi mathvariant=\"normal\">B</mi><mi mathvariant=\"normal\">e</mi><mi mathvariant=\"normal\">t</mi><mi mathvariant=\"normal\">a</mi></mrow><mo stretchy=\"false\">(</mo><mn>3</mn><mo separator=\"true\">,</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathrm{Beta}(3, 2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathrm\">B</span><span class=\"mord mathrm\">e</span><span class=\"mord mathrm\">t</span><span class=\"mord mathrm\">a</span></span><span class=\"mopen\">(</span><span class=\"mord\">3</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">2</span><span class=\"mclose\">)</span></span></span></span> which looks like\nthis.</p>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 583px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/081f6ddc219fb6675288a1b3fcb880fa/6d62d/beta32.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 74.66666666666667%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAAsTAAALEwEAmpwYAAABVklEQVQoz4WT23KDIBCG+/6vluYmV7lIlSgSrcYDBxUJS1exJqY6ZUaHRb799+SH+3cB7H35wEcplWWZ1nqbHQZgFK4EpNyGGWNd122gg4bgApw7rSH8ckq+w1LKOI77vt+QRUCK2ehaS4INOE3TFezzvDH4zmZzOoEkdoL7G3vKE6kUeJ2XggEe0mg5HGEhxAr2Iki26m+1ISLOmGfYTdM8w/ZkkQNL3sk5cvpbBZjhlTL2Jrg4+9hurhDYOe9rhmmSLDBcQ+DN7nj03Vi2V/iGfZ7gcR5Sth4wWDZgrU8bBj3nXFVVGJK+bR2N7JQqTOt1Y631+8cokLgFVlJWdR2HYR5HvTEJpZxzHLsoivCNrrGceO1+v+MUj2Jl2dT1DPvFheh67bMwxqAUukCTUno8HoMgOJ1O5/MZzc/DgRCygvcW/jBlWeZ5jk5xnDCQoihazNG5HwGvamhZcRq/AAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"beta32\" title=\"beta32\" src=\"/static/081f6ddc219fb6675288a1b3fcb880fa/6d62d/beta32.png\" srcset=\"/static/081f6ddc219fb6675288a1b3fcb880fa/a9205/beta32.png 150w,\n/static/081f6ddc219fb6675288a1b3fcb880fa/a8a0d/beta32.png 300w,\n/static/081f6ddc219fb6675288a1b3fcb880fa/6d62d/beta32.png 583w\" sizes=\"(max-width: 583px) 100vw, 583px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n  </a>\n    </span>\n</p>\n<p>Now that we've observed a heads, we can similarly rule out the possibility that\n<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\theta = 0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span></span></span></span>, so the posterior density there also goes to zero. Furthermore,\nwe've seen more heads than tails, so the posterior density is skewed towards\nhigher values of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>. Let's suppose now that we saw ten tails and twenty\nheads, so the proportion of heads is the same, but the amount of data goes up.\nNow the posterior is <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mrow><mi mathvariant=\"normal\">B</mi><mi mathvariant=\"normal\">e</mi><mi mathvariant=\"normal\">t</mi><mi mathvariant=\"normal\">a</mi></mrow><mo stretchy=\"false\">(</mo><mn>21</mn><mo separator=\"true\">,</mo><mn>11</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathrm{Beta}(21, 11)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathrm\">B</span><span class=\"mord mathrm\">e</span><span class=\"mord mathrm\">t</span><span class=\"mord mathrm\">a</span></span><span class=\"mopen\">(</span><span class=\"mord\">2</span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">1</span><span class=\"mord\">1</span><span class=\"mclose\">)</span></span></span></span> which looks like this.</p>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 553px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/5026119ae12277ba7b5b06594eef67e4/d35da/beta2111.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 78.66666666666667%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAAsTAAALEwEAmpwYAAABZUlEQVQoz6VTYW+DIBD1//+xpZ9s0g+dmxW1MlsrWkUDKhw7xa1NR5cluxg8OB7v8XJ45h/hKaWGYXAXAeahrqC52vkjeJomIcQvx8PhTYeBm7ksy7ZtFxpw0HIO9AhZaqRwy3YzW3B5MTWDaw1l8b14A7dLPGVOiBlH4C3yO8BSyr7vnzKnsVET9L0bjJrdYAwpZubF5jnR+vHOTdNwzh2yMTo+W2V1RKFR6m+GWc1ZCm2zLpw+jM3vOLyqqlZmVIWFuw/IAd1awRWDnM7ZnfiZGT37SWuK86rZTtFzPOtBNjLXZWnocYxClRCdkIkcdBzpJJ7GUWk996+USKfO+RDspyTWX+3saa3TNM1INHCeJwklZOw6HI+UKgBKaZ7nuI9m2akotBQXSivGrMEe/gTqXu6Go5CDTexr2e12vu8HQbDdbl/3+zTLXjab9zC8gZ8+CQA8tq5rayq2Q9d1jDHbF1j9BDKwpsHogi8zAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"beta2111\" title=\"beta2111\" src=\"/static/5026119ae12277ba7b5b06594eef67e4/d35da/beta2111.png\" srcset=\"/static/5026119ae12277ba7b5b06594eef67e4/a9205/beta2111.png 150w,\n/static/5026119ae12277ba7b5b06594eef67e4/a8a0d/beta2111.png 300w,\n/static/5026119ae12277ba7b5b06594eef67e4/d35da/beta2111.png 553w\" sizes=\"(max-width: 553px) 100vw, 553px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n  </a>\n    </span>\n</p>\n<p>Now the mode of the posterior distribution is closer to the proportion of heads\nthat we've seen, and the variance has shrunk, i.e. we are more confident of the\nlikely range of values for <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>.</p>\n<p>This illustrates a generic fact about the posterior distribution and its\nrelationship with the prior, namely that as we observe more data, the influence\nof the prior distribution diminishes, and the posterior distribution starts to\nbecome more certain of the values of the parameters in the model.</p>\n<p>The following animation illustrates the coin flipping with two different choices\nof prior. In this case the data was generated by simulating coin flips with a\ncoin that had probability <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mfrac><mn>3</mn><mn>4</mn></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac 3 4</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.190108em;vertical-align:-0.345em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.845108em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">4</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">3</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.345em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span> of coming up heads. We compare a uniform\nprior and a <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">B</mi><mi mathvariant=\"normal\">e</mi><mi mathvariant=\"normal\">t</mi><mi mathvariant=\"normal\">a</mi><mo stretchy=\"false\">(</mo><mn>5</mn><mo separator=\"true\">,</mo><mn>11</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathrm{Beta(5,11)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathrm\">B</span><span class=\"mord mathrm\">e</span><span class=\"mord mathrm\">t</span><span class=\"mord mathrm\">a</span><span class=\"mopen\">(</span><span class=\"mord mathrm\">5</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathrm\">1</span><span class=\"mord mathrm\">1</span><span class=\"mclose\">)</span></span></span></span></span> prior that assumes the coin is much more\nlikely to come up tails. To begin with after only a few coin flips the\nposteriors differ significantly, but as we observe more data they get more and\nmore similar as the influence of the prior fades.</p>\n<p align=\"center\">\n  <img src=\"/56a8cf89da2d9191fcd686805ebf194c/animation.gif\" width=\"900\">\n</p>\n<h2>Inference with the posterior distribution</h2>\n<p>Supposing we've specified the prior and computed the posterior, how do we\nactually use it? This part turns out to be pretty natural. Most of the time we\nwill want average some quantity of interest over the posterior distribution of\nparameter values. For example, the posterior mean of the parameters is simply</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"double-struck\">E</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mo>∫</mo><mi>θ</mi><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo stretchy=\"false\">)</mo><mi>d</mi><mi>θ</mi><mi mathvariant=\"normal\">.</mi></mrow><annotation encoding=\"application/x-tex\">  \\mathbb{E}(\\theta | y) = \\int \\theta p(\\theta | y) d\\theta.</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathbb\">E</span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.22225em;vertical-align:-0.86225em;\"></span><span class=\"mop op-symbol large-op\" style=\"margin-right:0.44445em;position:relative;top:-0.0011249999999999316em;\">∫</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">.</span></span></span></span></span>\n<p>Similarly, if we wanted to calculate the probability of some new data <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>~</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\tilde y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8622999999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">~</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span>\ngiven observed data <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>, we would calculate</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>y</mi><mo>~</mo></mover><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mo>∫</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>y</mi><mo>~</mo></mover><mi mathvariant=\"normal\">∣</mi><mi>θ</mi><mo stretchy=\"false\">)</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo stretchy=\"false\">)</mo><mi>d</mi><mi>θ</mi><mi mathvariant=\"normal\">.</mi></mrow><annotation encoding=\"application/x-tex\">  p(\\tilde y | y) = \\int p(\\tilde y | \\theta) p(\\theta | y) d\\theta.</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">~</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.22225em;vertical-align:-0.86225em;\"></span><span class=\"mop op-symbol large-op\" style=\"margin-right:0.44445em;position:relative;top:-0.0011249999999999316em;\">∫</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">~</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">.</span></span></span></span></span>\n<p>There is a natural interpretation of the right hand side in this context. We\ncalculate the probability (density) of the new data <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>~</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\tilde y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8622999999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">~</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span> for every\npossible value of the parameters <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>, the <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>y</mi><mo>~</mo></mover><mi mathvariant=\"normal\">∣</mi><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\tilde y \\vert \\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">~</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span> term;\nthen we compute the weighted average of those where the terms involving more\nlikely parameter values (according to the posterior) contribute more to the\naverage.</p>\n<p>We can also calculate probabilities of parameters taking particular values.\nSuppose in our coin flipping example we want to know whether\n<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi><mo>&gt;</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><annotation encoding=\"application/x-tex\">\\theta &gt; \\frac 1 2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.73354em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">&gt;</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.190108em;vertical-align:-0.345em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.845108em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.345em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span>. We can just calculate the probability directly by\nintegrating</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mrow><mo fence=\"true\">(</mo><mi>θ</mi><mo>&gt;</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mspace width=\"0.5em\"/><mo fence=\"false\">∣</mo><mspace width=\"0.5em\"/><mi>y</mi><mo fence=\"true\">)</mo></mrow><mo>=</mo><msubsup><mo>∫</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mn>1</mn></msubsup><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo stretchy=\"false\">)</mo><mi>d</mi><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">    P\\left(\\theta &gt; \\frac 1 2 \\hspace{5pt} \\bigg | \\hspace{5pt} y \\right) = \\int_{\\frac 1 2}^1 p(\\theta | y) d\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:2.43em;vertical-align:-0.95003em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">(</span></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">&gt;</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.32144em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\">2</span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\">1</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.5em;\"></span><span class=\"mord\"><span class=\"delimsizing mult\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.4799700000000002em;\"><span style=\"top:-1.65598em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span class=\"delimsizinginner delim-size1\"><span>∣</span></span></span><span style=\"top:-2.25698em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span class=\"delimsizinginner delim-size1\"><span>∣</span></span></span><span style=\"top:-2.85798em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span class=\"delimsizinginner delim-size1\"><span>∣</span></span></span><span style=\"top:-2.87897em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span class=\"delimsizinginner delim-size1\"><span>∣</span></span></span><span style=\"top:-3.47997em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span class=\"delimsizinginner delim-size1\"><span>∣</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500199999999999em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.5em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">)</span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.7167579999999996em;vertical-align:-1.15275em;\"></span><span class=\"mop\"><span class=\"mop op-symbol large-op\" style=\"margin-right:0.44445em;position:relative;top:-0.0011249999999999316em;\">∫</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5640079999999998em;\"><span style=\"top:-2.08805em;margin-left:-0.44445em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8443142857142858em;\"><span style=\"top:-2.656em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\">2</span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.344em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span></span></span></span><span style=\"top:-4.1129em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.15275em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span></span>\n<p>The right hand side can be expressed in terms of the cumulative distribution\nfunction (CDF) of the beta distribution. Since the beta distribution is a\ncommonly used and well understood one-dimensional distribution, numerical\nimplementations of the CDF are readily available. For example, let's suppose we\nflip the coin thirty times, and observe twenty heads and ten tails. Based on the\ncalculations we did above our posterior distribution is <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mrow><mi mathvariant=\"normal\">B</mi><mi mathvariant=\"normal\">e</mi><mi mathvariant=\"normal\">t</mi><mi mathvariant=\"normal\">a</mi></mrow><mo stretchy=\"false\">(</mo><mn>21</mn><mo separator=\"true\">,</mo><mn>11</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathrm{Beta}(21, 11)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathrm\">B</span><span class=\"mord mathrm\">e</span><span class=\"mord mathrm\">t</span><span class=\"mord mathrm\">a</span></span><span class=\"mopen\">(</span><span class=\"mord\">2</span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">1</span><span class=\"mord\">1</span><span class=\"mclose\">)</span></span></span></span>,\nso in Python with SciPy we can do</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token keyword\">from</span> scipy<span class=\"token punctuation\">.</span>stats<span class=\"token punctuation\">.</span>distributions <span class=\"token keyword\">import</span> beta\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token number\">1</span> <span class=\"token operator\">-</span> beta<span class=\"token punctuation\">.</span>cdf<span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">21</span><span class=\"token punctuation\">,</span> <span class=\"token number\">11</span><span class=\"token punctuation\">)</span>\n<span class=\"token number\">0.9646222270093858</span></code></pre></div>\n<h2>Inference with posterior samples</h2>\n<p>In general, particularly with high-dimensional posterior distributions where we\nhave many parameters in the model, approximating the integrals we need to\nevaluate to do inference will be much harder. It turns out though, that we can\nget good approximations to these integrals if we have access to samples from the\nposterior distribution. For example, this code snippet draws ten thousands\nsamples from the posterior, then calculates the proportion that are greater than\n<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">0.5</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">5</span></span></span></span>. The answer is a close approximation to the answer obtained from the CDF\nabove.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> samples <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>beta<span class=\"token punctuation\">(</span><span class=\"token number\">21</span><span class=\"token punctuation\">,</span> <span class=\"token number\">11</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10000</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token punctuation\">(</span>samples <span class=\"token operator\">></span> <span class=\"token number\">0.5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token number\">0.9653</span></code></pre></div>\n<p>This is a specific example of using a <a href=\"https://en.wikipedia.org/wiki/Monte_Carlo_integration\">Monte Carlo estimate</a> to approximate\nan integral. In general, if we have an integral of the form</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>∫</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mo stretchy=\"false\">)</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo stretchy=\"false\">)</mo><mi>d</mi><mi>θ</mi><mo separator=\"true\">,</mo></mrow><annotation encoding=\"application/x-tex\">  \\int f(\\theta) p(\\theta | y) d\\theta,</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:2.22225em;vertical-align:-0.86225em;\"></span><span class=\"mop op-symbol large-op\" style=\"margin-right:0.44445em;position:relative;top:-0.0011249999999999316em;\">∫</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mpunct\">,</span></span></span></span></span>\n<p>we can approximate that with a sum of the form</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mfrac><mn>1</mn><mi>S</mi></mfrac><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mi>f</mi><mo stretchy=\"false\">(</mo><msup><mi>θ</mi><mi>s</mi></msup><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo></mrow><annotation encoding=\"application/x-tex\">  \\frac 1 S \\sum_{s=1}^S f(\\theta^s),</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:3.0954490000000003em;vertical-align:-1.267113em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.32144em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\">1</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.8283360000000002em;\"><span style=\"top:-1.882887em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">s</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3000050000000005em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05764em;\">S</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.267113em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7143919999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">s</span></span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span></span></span></span></span>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>θ</mi><mi>s</mi></msup><mspace width=\"0.5em\"/><mi>s</mi><mo>=</mo><mn>1</mn><mo separator=\"true\">,</mo><mo>…</mo><mo separator=\"true\">,</mo><mi>S</mi></mrow><annotation encoding=\"application/x-tex\">\\theta^s \\hspace{5pt} s = 1, \\dots, S</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">s</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.5em;\"></span><span class=\"mord mathdefault\">s</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\">…</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span></span></span></span> are samples from the distribution\ndefined by <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta \\vert y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span>. That is, we draw posterior samples then evaluate\nthe integrand on those samples and average. If you are familiar with numerical\nintegration methods, you can think of this like a grid approximation with a\ndynamic grid, where the grid density is higher in regions where the probability\ndensity is higher.</p>\n<p>Given a set of samples <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>θ</mi><mi>s</mi></msup><mspace width=\"0.5em\"/><mi>s</mi><mo>=</mo><mn>1</mn><mo separator=\"true\">,</mo><mo>…</mo><mo separator=\"true\">,</mo><mi>S</mi></mrow><annotation encoding=\"application/x-tex\">\\theta^s \\hspace{5pt} s = 1, \\dots, S</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">s</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.5em;\"></span><span class=\"mord mathdefault\">s</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\">…</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span></span></span></span> and using the\nintegral formulas above, we can approximate posterior means</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mfrac><mn>1</mn><mi>S</mi></mfrac><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><msup><mi>θ</mi><mi>s</mi></msup><mo separator=\"true\">,</mo></mrow><annotation encoding=\"application/x-tex\">  \\frac 1 S \\sum_{s=1}^S \\theta^s,</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:3.0954490000000003em;vertical-align:-1.267113em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.32144em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\">1</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.8283360000000002em;\"><span style=\"top:-1.882887em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">s</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3000050000000005em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05764em;\">S</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.267113em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7143919999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">s</span></span></span></span></span></span></span></span><span class=\"mpunct\">,</span></span></span></span></span>\n<p>and the posterior predictive distribution (the probability of observing new data\n<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>~</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\tilde y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8622999999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">~</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span>)</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mfrac><mn>1</mn><mi>S</mi></mfrac><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mi>p</mi><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>y</mi><mo>~</mo></mover><mi mathvariant=\"normal\">∣</mi><msup><mi>θ</mi><mi>s</mi></msup><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">.</mi></mrow><annotation encoding=\"application/x-tex\">  \\frac 1 S \\sum_{s=1}^S p(\\tilde y | \\theta^s).</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:3.0954490000000003em;vertical-align:-1.267113em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.32144em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\">1</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.8283360000000002em;\"><span style=\"top:-1.882887em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">s</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3000050000000005em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05764em;\">S</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.267113em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">~</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7143919999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">s</span></span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mord\">.</span></span></span></span></span>\n<p>In our case, with the samples we drew before, our approximation of the posterior\nmean is</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> samples<span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token number\">0.6562683695235446</span></code></pre></div>\n<p>Similarly, if we wanted to predict the probability of getting a heads, then\ntails (in that order) on the next two coin flips, we have\n<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mtext>heads then tails</mtext><mi mathvariant=\"normal\">∣</mi><mi>θ</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>θ</mi><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\text{heads then tails} \\vert \\theta) = \\theta (1 - \\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">heads then tails</span></span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mopen\">(</span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span>, which we can\napproximate with the samples as follows</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token punctuation\">(</span>samples <span class=\"token operator\">*</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span> <span class=\"token operator\">-</span> samples<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token number\">0.2187</span></code></pre></div>\n<p>Just about any inference we might like to make about the parameters in our model\ncan be calculated in a natural way like this, provided you can draw samples from\nthe posterior distribution. Though it is out of scope for this particular series\nof blog posts, a popular family of algorithms for drawing samples from arbitrary\nprobability distirbutions are Markov chain Monte Carlo (MCMC) algorithms. One of\nthe nice features of these algorithms is that they often don't require you to\nnormalise the posterior distribution, it suffices to know it up to a\nmultiplicative constant. In the context of Bayesian statistics where we\ncalculate the posterior as the product of two terms that ordinarily would then\nneed to be normalised, this is extremely convenient! If you are interested in\nseeing how we could have used an MCMC sampler to sample from our posterior,\ncheck out the addendum about drawing posterior samples using Stan.</p>\n<p>In the next post we'll describe hierarchical models of which multi-level\nregression is an example.</p>\n<h2>Addendum: drawing posterior samples using Stan</h2>\n<p>In the coin flipping example above, we knew the precise form of the posterior,\nand it happened to be something that could easily be sampled from using\nfunctions built into widely used libraries like NumPy. If we had a more\ncomplicated posterior we would probably need to turn to specialised sampling\ntools. In this addendum we'll take a quick look at how we can draw samples from\nthe posterior using Stan and PyStan. We'll keep using the coin flipping example\nso we can compare to earlier results, but everything we will do generalises to\nmore complicated models and distributions.</p>\n<p>Stan implements a powerful sampling algorithm called Hamiltonian Monte Carlo\n(HMC). The details of how it works are beyond the scope of this post, but there\nis one important thing we should know in order to understand the output of Stan.\nHMC, and MCMC algorithms more generally, draw correlated, not fully independent\nsamples. Provided we properly account for the correlation in the samples though,\nthis isn't a barrier to making inferences.</p>\n<p>Stan has its own modelling language which we use to specify the model. The Stan\ncompiler <code class=\"language-text\">stanc</code> will transpile the Stan modelling language to optimized C++\ncode, then compile the C++ code for us. The result is a very efficient sampling\nalgorithm for the posterior distribution of our model. We won't discuss the\nsyntax or features of the Stan modelling language here, check out the very good\n<a href=\"https://mc-stan.org/users/documentation/\">documentation</a> if you would like to know more.</p>\n<p>We need to define the data, parameters and then specify the prior and sampling\ndistribution. We do so in three blocks of code labelled <code class=\"language-text\">data</code>, <code class=\"language-text\">parameters</code> and\n<code class=\"language-text\">model</code>. Stan does the rest by computing the posterior and drawing samples using\nHMC. Here is the full model we are going to use.</p>\n<div class=\"gatsby-highlight\" data-language=\"stan\"><pre class=\"language-stan\"><code class=\"language-stan\"><span class=\"token keyword\">data</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token keyword\">int</span><span class=\"token constraint\"><span class=\"token punctuation\">&lt;</span><span class=\"token property\">lower</span><span class=\"token operator\">=</span><span class=\"token expression\"><span class=\"token number\">0</span></span><span class=\"token punctuation\">></span></span> N<span class=\"token punctuation\">;</span>  <span class=\"token comment\">// number of coin flips</span>\n  <span class=\"token keyword\">int</span><span class=\"token constraint\"><span class=\"token punctuation\">&lt;</span><span class=\"token property\">lower</span><span class=\"token operator\">=</span><span class=\"token expression\"><span class=\"token number\">0</span></span><span class=\"token punctuation\">,</span> <span class=\"token property\">upper</span><span class=\"token operator\">=</span><span class=\"token expression\">N</span><span class=\"token punctuation\">></span></span> y<span class=\"token punctuation\">;</span>  <span class=\"token comment\">// number of heads</span>\n<span class=\"token punctuation\">}</span>\n<span class=\"token keyword\">parameters</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token keyword\">real</span><span class=\"token constraint\"><span class=\"token punctuation\">&lt;</span><span class=\"token property\">lower</span><span class=\"token operator\">=</span><span class=\"token expression\"><span class=\"token number\">0</span></span><span class=\"token punctuation\">,</span> <span class=\"token property\">upper</span><span class=\"token operator\">=</span><span class=\"token expression\"><span class=\"token number\">1</span></span><span class=\"token punctuation\">></span></span> theta<span class=\"token punctuation\">;</span>  <span class=\"token comment\">// probability of heads</span>\n<span class=\"token punctuation\">}</span>\n<span class=\"token keyword\">model</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token comment\">// the prior</span>\n  theta <span class=\"token operator\">~</span> <span class=\"token function\">uniform</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  <span class=\"token comment\">// the sampling distribution</span>\n  y <span class=\"token operator\">~</span> <span class=\"token function\">binomial</span><span class=\"token punctuation\">(</span>N<span class=\"token punctuation\">,</span> theta<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>Let's look at each block in turn. First the <code class=\"language-text\">data</code> block.</p>\n<div class=\"gatsby-highlight\" data-language=\"stan\"><pre class=\"language-stan\"><code class=\"language-stan\"><span class=\"token keyword\">data</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token keyword\">int</span><span class=\"token constraint\"><span class=\"token punctuation\">&lt;</span><span class=\"token property\">lower</span><span class=\"token operator\">=</span><span class=\"token expression\"><span class=\"token number\">0</span></span><span class=\"token punctuation\">></span></span> N<span class=\"token punctuation\">;</span>  <span class=\"token comment\">// number of coin flips</span>\n  <span class=\"token keyword\">int</span><span class=\"token constraint\"><span class=\"token punctuation\">&lt;</span><span class=\"token property\">lower</span><span class=\"token operator\">=</span><span class=\"token expression\"><span class=\"token number\">0</span></span><span class=\"token punctuation\">,</span> <span class=\"token property\">upper</span><span class=\"token operator\">=</span><span class=\"token expression\">N</span><span class=\"token punctuation\">></span></span> y<span class=\"token punctuation\">;</span>  <span class=\"token comment\">// number of heads</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>We specify that Stan should expect two pieces of information: the number of\ntimes we flipped the coin <code class=\"language-text\">N</code>, and the number of times it came up heads <code class=\"language-text\">y</code>.\nStan is a typed language, so we specify that both <code class=\"language-text\">N</code> and <code class=\"language-text\">y</code> are integers.\nAdditionally, Stan allows us to place constraints on the data which will be\nchecked at runtime. We specify that we can't have a negative number of coin\nflips, and the number of heads should be non-negative and less than <code class=\"language-text\">N</code>.</p>\n<p>Next the parameters block</p>\n<div class=\"gatsby-highlight\" data-language=\"stan\"><pre class=\"language-stan\"><code class=\"language-stan\"><span class=\"token keyword\">parameters</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token keyword\">real</span><span class=\"token constraint\"><span class=\"token punctuation\">&lt;</span><span class=\"token property\">lower</span><span class=\"token operator\">=</span><span class=\"token expression\"><span class=\"token number\">0</span></span><span class=\"token punctuation\">,</span> <span class=\"token property\">upper</span><span class=\"token operator\">=</span><span class=\"token expression\"><span class=\"token number\">1</span></span><span class=\"token punctuation\">></span></span> theta<span class=\"token punctuation\">;</span>  <span class=\"token comment\">// probability of heads</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>We have only one parameter in our model, <code class=\"language-text\">theta</code>. This is the probability that a\nsingle flip will come up heads. As such it should be a real number between 0\nand 1.</p>\n<p>Finally the model</p>\n<div class=\"gatsby-highlight\" data-language=\"stan\"><pre class=\"language-stan\"><code class=\"language-stan\"><span class=\"token keyword\">model</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token comment\">// the prior</span>\n  theta <span class=\"token operator\">~</span> <span class=\"token function\">uniform</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  <span class=\"token comment\">// the sampling distribution</span>\n  y <span class=\"token operator\">~</span> <span class=\"token function\">binomial</span><span class=\"token punctuation\">(</span>N<span class=\"token punctuation\">,</span> theta<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>We specify a uniform prior on theta (which is actually redundant, if we hadn't\nspecified a prior on <code class=\"language-text\">theta</code> then Stan would have automatically chosen a uniform\nprior for us), and a binomial sampling distribution for <code class=\"language-text\">y</code>. The Stan syntax\nmirrors mathematical notation quite nicely, so the model specification is quite\nreadable.</p>\n<p>To compile the model and draw samples we're going to use the Python interface\nPyStan. There are however many alternatives including interfaces for R, Matlab,\nJulia and the command line. Details can be found in the <a href=\"https://mc-stan.org/users/documentation/\">Stan\ndocumentation</a>. To get started with PyStan, install it with <code class=\"language-text\">pip</code>.</p>\n<div class=\"gatsby-highlight\" data-language=\"sh\"><pre class=\"language-sh\"><code class=\"language-sh\">pip install pystan</code></pre></div>\n<p>Now we can load the model as a <code class=\"language-text\">pystan.StanModel</code> object and use the <code class=\"language-text\">sampling</code>\nmethod to draw samples. Data is passed to the model using a Python dictionary as\nfollows. We'll assume twenty heads and ten tails like we did previously.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> pystan\n\nsm <span class=\"token operator\">=</span> pystan<span class=\"token punctuation\">.</span>StanModel<span class=\"token punctuation\">(</span><span class=\"token builtin\">file</span><span class=\"token operator\">=</span><span class=\"token string\">\"model.stan\"</span><span class=\"token punctuation\">)</span>\nfit <span class=\"token operator\">=</span> sm<span class=\"token punctuation\">.</span>sampling<span class=\"token punctuation\">(</span>data<span class=\"token operator\">=</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"N\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">30</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"y\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">20</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">iter</span><span class=\"token operator\">=</span><span class=\"token number\">5000</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>If you run the above code snippet, Stan will create four chains (the default),\nand draw 5000 samples from each. MCMC samplers can be sensitive to initial\nconditions, so running multiple independent chains helps us assess whether the\nchains have converged to the correct distributions. For each chain we discard\nthe first 50% of samples as warmup, leaving us with a total of 10000 post-warmup\ndraws from the four chains. Stan prints a summary of the sampling like the one\nbelow.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Inference for Stan model: anon_model_bd7785d44829c52cfe28d5321faadbc2.\n4 chains, each with iter=5000; warmup=2500; thin=1;\npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n        mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\ntheta   0.66  1.3e-3   0.08   0.49    0.6   0.66   0.71   0.81   3822    1.0\nlp__  -21.09    0.01   0.69 -23.09 -21.25 -20.82 -20.64 -20.59   3845    1.0\n\nSamples were drawn using NUTS at Sun Nov 10 10:12:21 2019.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at\nconvergence, Rhat=1).</code></pre></div>\n<p>For each parameter in our model, in this case <code class=\"language-text\">theta</code>, we get an estimate of the\nposterior mean, the standard error, the standard deviation and various\npercentiles. This means at a glance we can read off some basic information about\n<code class=\"language-text\">theta</code> such as the posterior mean, which in this case is <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0.66</mn></mrow><annotation encoding=\"application/x-tex\">0.66</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">6</span><span class=\"mord\">6</span></span></span></span>. We can also\nsee that a central 95% credible interval for <code class=\"language-text\">theta</code> would be <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0.49</mn></mrow><annotation encoding=\"application/x-tex\">0.49</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">4</span><span class=\"mord\">9</span></span></span></span> to <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0.81</mn></mrow><annotation encoding=\"application/x-tex\">0.81</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">8</span><span class=\"mord\">1</span></span></span></span>,\nso there's still a fairly wide range of credible values.</p>\n<p>The final two columns <code class=\"language-text\">n_eff</code> and <code class=\"language-text\">Rhat</code> help us understand how correlated the\nsamples are, and how well the sampler converged. <code class=\"language-text\">n_eff</code> counts the \"effective\"\nnumber of samples, roughly speaking we can expect the correlated samples drawn\nby Stan to have the utility of this number of independent samples. <code class=\"language-text\">Rhat</code>\nmeasures how well the chains have converged and whether they have converged to\nthe same thing. An <code class=\"language-text\">Rhat</code> value of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">.</span><span class=\"mord\">0</span></span></span></span> is necessary but not sufficient for\nconvergence, i.e. if the chains have converged then <code class=\"language-text\">Rhat</code> will be <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">.</span><span class=\"mord\">0</span></span></span></span>, though\nif <code class=\"language-text\">Rhat</code> is <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">.</span><span class=\"mord\">0</span></span></span></span> convergence isn't guaranteed. In our case we have a large\nnumber of effective samples and an <code class=\"language-text\">Rhat</code> of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">.</span><span class=\"mord\">0</span></span></span></span>, so sampling appears to have\nbeen successful.</p>\n<p>We can get the sampled values of <code class=\"language-text\">theta</code> and do computations with them manually,\nfor example, let's calculate the probability that <code class=\"language-text\">theta</code> is greater than <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">0.5</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">5</span></span></span></span>\nand compare to our earlier answers</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> samples <span class=\"token operator\">=</span> fit<span class=\"token punctuation\">.</span>extract<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"theta\"</span><span class=\"token punctuation\">]</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token punctuation\">(</span>samples <span class=\"token operator\">></span> <span class=\"token number\">0.5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token number\">0.9685</span></code></pre></div>\n<p>or the probability we next flip a heads, then tails</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token punctuation\">(</span>samples <span class=\"token operator\">*</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span> <span class=\"token operator\">-</span> samples<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token number\">0.21900253298714248</span></code></pre></div>\n<p>Both answers are in line with the results we saw earlier, but this time around\nwe didn't have to explicitly compute the posterior distribution! Stan is able to\ndo all of the heavy lifting for us. When dealing with more complicated\ndistributions than what we've been considering, this turns out to be crucial for\nmaking inference tractable.</p>","excerpt":"This is the second post in a series on election modelling; specifically\nmulti-level regression with poststratification (MRP) and its…"}},"pageContext":{"next":{"frontmatter":{"path":"/blog/election-modelling-part-3","title":"Election Modelling - Part 3","tags":["bayesian statistics","politics"]},"fields":{"collection":"posts"},"fileAbsolutePath":"/home/runner/work/tcbegley.github.io/tcbegley.github.io/src/posts/election-modelling-3.md"},"previous":{"frontmatter":{"path":"/blog/election-modelling-part-1","title":"Election Modelling - Part 1","tags":["bayesian statistics","politics"]},"fields":{"collection":"posts"},"fileAbsolutePath":"/home/runner/work/tcbegley.github.io/tcbegley.github.io/src/posts/election-modelling-1.md"}}},"staticQueryHashes":["1425477374","3128451518"]}