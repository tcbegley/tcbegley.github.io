{"componentChunkName":"component---src-templates-post-js","path":"/blog/election-modelling-part-2","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Election Modelling - Part 2","date":"11 November 2019","path":"/blog/election-modelling-part-2","author":"Tom","excerpt":"This is the second post in a series on election modelling; specifically multi-level regression with poststratification (MRP) and its successful use by YouGov in the 2017 general election...","tags":["bayesian statistics","politics","stan"],"coverImage":null},"id":"4915864e-5eff-5c2d-bb3a-02b18c579ee2","html":"<p>This is the second post in a series on election modelling; specifically multi-level regression with poststratification (MRP) and its successful use by YouGov in the 2017 general election.</p>\n<p>In this post we are going cover technical preliminaries for understanding MRP. In particular we need to know a few things about Bayesian statistics, and computational inference.</p>\n<h2>Bayesian statistics</h2>\n<p>Bayesian statistics differs from the more common frequentist approach in a subtle way that has far reaching consequences. It comes down to a different interpretation of probabilities. Specifically Bayesians interpret probability as quantifying the state of our knowledge, whereas frequentists interpret probability as frequencies of particular outcomes in repeated trials.</p>\n<p>Under a Bayesian interpretation of probability, it makes sense to make probabilistic statements about fixed, unknown quantities such as model parameters; the probability quantifies our knowledge about the parameter.</p>\n<p>Under a frequentist interpretation on the other hand it does not make sense, since parameters are assumed fixed we can't repeat trials / experiments and calculate frequencies. These blog posts make no attempt to resolve the (at times passionate) dispute between these two schools of thought, we're just interested in learning the basics of the Bayesian approach.</p>\n<p>Because we are able to make probabilistic statements about model parameters when we have our Bayesian hats on, we can formulate and work with a joint probability model for data and parameters. The recipe for doing this comes from Bayes' theorem. Let's denote by <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> model parameters, and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span> data. Then</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mi mathvariant=\"normal\">∣</mi><mi>θ</mi><mo stretchy=\"false\">)</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mfrac><mo>∝</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mi mathvariant=\"normal\">∣</mi><mi>θ</mi><mo stretchy=\"false\">)</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">    p(\\theta | y) = \\frac{p(y|\\theta)p(\\theta)}{p(y)} \\propto p(y | \\theta) p(\\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.363em;vertical-align:-0.936em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∝</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span></span>\n<p>Let's look at each term individually:</p>\n<ul>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta \\vert y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span> is called the <em>posterior distribution</em>. It is the distribution of model parameters conditioned on the data, and as such represents what we know about the model parameters having observed the data <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>. This is what we want to calculate so that we may do inference.</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mi mathvariant=\"normal\">∣</mi><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(y \\vert \\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span> is called the <em>sampling distribution</em> when viewed as a distribution over <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>, or the <em>likelihood</em> when viewed as a function of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>. It describes how data is generated given fixed model parameters.</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span> is called the <em>prior distribution</em>. It is the distribution of model parameters in the absence of any other information about <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>. It represents what we know about the model parameters before having observed any data.</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span> is known as the <em>evidence</em> or <em>data marginal</em>. We typically don't worry about it so much because it can be recovered by integrating the right hand side with respect to <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>, and is just a normalising constant.</li>\n</ul>\n<p>The Bayesian workflow proceeds as follows: we specify a joint probability distribution on our data and parameters by specifying a prior and a sampling distribution. We then condition on the data to get the posterior, and use that to make inferences about the parameters. We can then test our inferences and repeat the process to refine our model.</p>\n<h2>Example: coin flipping</h2>\n<p>This can all be a little bit confusing if you've not seen it before, so it's useful to work through a concrete example. Let's imagine we have a coin, which is not necessarily fair, and we want to infer the probability that the coin shows heads when flipped. We'll collect data by repeatedly flipping the coin. Let's denote by <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">n</span></span></span></span> the number of coin flips, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span> the the number of heads, and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> the probability of getting heads on a single flip.</p>\n<p>The first thing we need to do is specify a joint probability distribution over data and parameters via the prior and sampling distributions. The sampling distribution is relatively straightforward. Recall it describes how the outcome of the coin flip <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span> is generated given the parameter <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>. In this case <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span> should count the number of \"successful\" outcomes (i.e. flips where we get heads) in <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">n</span></span></span></span> independent flips where there is a fixed probability <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> of \"success\". This is the same thing as saying that <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span> is Binomially distributed with <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">n</span></span></span></span> trials and probability <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>, that is to say our sampling distribution is <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi><mi mathvariant=\"normal\">∣</mi><mi>θ</mi><mo>∼</mo><mrow><mi mathvariant=\"normal\">B</mi><mi mathvariant=\"normal\">i</mi><mi mathvariant=\"normal\">n</mi><mi mathvariant=\"normal\">o</mi><mi mathvariant=\"normal\">m</mi><mi mathvariant=\"normal\">i</mi><mi mathvariant=\"normal\">a</mi><mi mathvariant=\"normal\">l</mi></mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo separator=\"true\">,</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">y \\vert \\theta \\sim \\mathrm{Binomial}(n, \\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∼</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathrm\">B</span><span class=\"mord mathrm\">i</span><span class=\"mord mathrm\">n</span><span class=\"mord mathrm\">o</span><span class=\"mord mathrm\">m</span><span class=\"mord mathrm\">i</span><span class=\"mord mathrm\">a</span><span class=\"mord mathrm\">l</span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">n</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span>.</p>\n<p>Next we need to specify a prior on <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>, which quantifies our knowledge about <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> in the absence of any observed data. This comes a little less naturally, and many critics of Bayesian statistics say that the choice is subjective and biases inference with the statistician's own beliefs or predjudices. We can view it as just another modelling assumption and ignore these concerns for now. Still, what is a reasonable choice? We can take a maximally unopinionated stance and say that without having flipped the coin, we have no way of knowing what the chances of getting heads is, and so we consider any value of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> to be equally likely. Since <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> is meant to represent a probability, it must be between <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span></span></span></span>, so we choose <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi><mo>∼</mo><mrow><mi mathvariant=\"normal\">U</mi><mi mathvariant=\"normal\">n</mi><mi mathvariant=\"normal\">i</mi><mi mathvariant=\"normal\">f</mi><mi mathvariant=\"normal\">o</mi><mi mathvariant=\"normal\">r</mi><mi mathvariant=\"normal\">m</mi></mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\theta \\sim \\mathrm{Uniform}(0, 1)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∼</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathrm\">U</span><span class=\"mord mathrm\">n</span><span class=\"mord mathrm\">i</span><span class=\"mord mathrm\" style=\"margin-right:0.07778em;\">f</span><span class=\"mord mathrm\">o</span><span class=\"mord mathrm\">r</span><span class=\"mord mathrm\">m</span></span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">1</span><span class=\"mclose\">)</span></span></span></span> as our prior.</p>\n<p>To make inferences, we need to compute the posterior, which means multiplying the prior and sampling distributions together, and normalising. In this case though when multiplying the distributions we find</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo stretchy=\"false\">)</mo><mo>∝</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mi mathvariant=\"normal\">∣</mi><mi>θ</mi><mo stretchy=\"false\">)</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mo stretchy=\"false\">)</mo><mo>∝</mo><msup><mi>θ</mi><mi>y</mi></msup><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mi>θ</mi><msup><mo stretchy=\"false\">)</mo><mrow><mi>n</mi><mo>−</mo><mi>y</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">    p(\\theta | y) \\propto p(y | \\theta) p(\\theta) \\propto \\theta ^ y (1 - \\theta)^{n-y}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∝</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∝</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.714392em;\"><span style=\"top:-3.1130000000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">y</span></span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.071331em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.821331em;\"><span style=\"top:-3.1130000000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">n</span><span class=\"mbin mtight\">−</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03588em;\">y</span></span></span></span></span></span></span></span></span></span></span></span></span>\n<p>We discard any constants independent of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> since they would be cancelled in the normalisation anyway. On inspection we notice that the right hand side is an unnormalised beta distribution in <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>, and so we deduce that <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo>∼</mo><mrow><mi mathvariant=\"normal\">B</mi><mi mathvariant=\"normal\">e</mi><mi mathvariant=\"normal\">t</mi><mi mathvariant=\"normal\">a</mi></mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo>+</mo><mn>1</mn><mo separator=\"true\">,</mo><mi>n</mi><mo>−</mo><mi>y</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\theta \\vert y \\sim \\mathrm{Beta}(y + 1, n - y + 1)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∼</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathrm\">B</span><span class=\"mord mathrm\">e</span><span class=\"mord mathrm\">t</span><span class=\"mord mathrm\">a</span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8388800000000001em;vertical-align:-0.19444em;\"></span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">n</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.7777700000000001em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">1</span><span class=\"mclose\">)</span></span></span></span>.</p>\n<p>We'll look at how to use this distribution to make inferences in the next section, but first let's look at how the posterior changes as we observe coin flips. To begin with, before we observe any data, the posterior is just the prior.</p>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 566px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/8deac3ab7ebe05193ebacf69c026bbf2/7ef45/beta11.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 77.33333333333333%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA10lEQVQoz6WSzQ6DIBCEff+H82S0Hjy0pUkR0IgKiAodpWnTQ39CN0SH1c8ddk38H5E459q2lV3nGHXk5C7nL4scveBP2BjT9/1qtP9pKW+nl8pN00DE2B6GoYPtaFhKGQnDM5xHwtbarWHrGgMLITjnkXNGTaVUZGXGGI4dYLfHOxGYh9jgZVniK1/3wKi11hgbmod/DsODhsB1HMdpmpAPAhnUu8O4YU8IgYD/uq4hKKVoJESe53iEF4qiQBJDLcsSX9ngz8bmea6qKsuywx4wCJ2maYBvPUpvnWqZu60AAAAASUVORK5CYII=&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"beta11\" title=\"beta11\" src=\"/static/8deac3ab7ebe05193ebacf69c026bbf2/7ef45/beta11.png\" srcset=\"/static/8deac3ab7ebe05193ebacf69c026bbf2/a9205/beta11.png 150w,\n/static/8deac3ab7ebe05193ebacf69c026bbf2/a8a0d/beta11.png 300w,\n/static/8deac3ab7ebe05193ebacf69c026bbf2/7ef45/beta11.png 566w\" sizes=\"(max-width: 566px) 100vw, 566px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n  </a>\n    </span>\n</p>\n<p>If we observed a tails, the posterior becomes <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mrow><mi mathvariant=\"normal\">B</mi><mi mathvariant=\"normal\">e</mi><mi mathvariant=\"normal\">t</mi><mi mathvariant=\"normal\">a</mi></mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo separator=\"true\">,</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathrm{Beta}(1, 2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathrm\">B</span><span class=\"mord mathrm\">e</span><span class=\"mord mathrm\">t</span><span class=\"mord mathrm\">a</span></span><span class=\"mopen\">(</span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">2</span><span class=\"mclose\">)</span></span></span></span> which looks like this.</p>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 568px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/a0ac649e37e365c2f2fc3f4e0981803c/d63a8/beta12.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 76.66666666666666%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAAsTAAALEwEAmpwYAAABNUlEQVQoz5WSC4+CMBCE+f9/z1xUpOKhBAjP8hCxdHrToCd3np42IWza/XZmu3XMfAHmneXwK4oijmMjK5xOb5Ww8DAMsq41YbFB27zOO9Ov6zpN4HzGTiCJX+zCwmVZRlH0nY0kwm4Lpf7lL8p932utb3zbQLioyue8M3lO0/T3nSuFYIfwYKAflbBw27Y/4FkJZCl8zzyYwp3t+7Gfemw9FPllZ1bCwhxyGIZ/tzWlAvSP/afR49zCRfl4PI7jeM3HPMBVDWWhxcZ07d0jkRJPp4KpqfOgtxtexFTbwlmW0Xae59RvmoZjZ1DXNQPeBXc4DqWUrKrWGtTNIaDezTbPgiCgODH7zo0hnCQJA9d1Pc+jtcViwZxayo/Viko3+KFbgI9vvV6zhO/7jIUQy+WSpnj6BWKmbtPEkQ01AAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"beta12\" title=\"beta12\" src=\"/static/a0ac649e37e365c2f2fc3f4e0981803c/d63a8/beta12.png\" srcset=\"/static/a0ac649e37e365c2f2fc3f4e0981803c/a9205/beta12.png 150w,\n/static/a0ac649e37e365c2f2fc3f4e0981803c/a8a0d/beta12.png 300w,\n/static/a0ac649e37e365c2f2fc3f4e0981803c/d63a8/beta12.png 568w\" sizes=\"(max-width: 568px) 100vw, 568px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n  </a>\n    </span>\n</p>\n<p>We can see the posterior probability density of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\theta = 1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span></span></span></span> immediately goes to <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span></span></span></span>. This makes sense because we just observed tails, so we can rule out the possibility that the coin is certain to come up heads. Similarly we are much more likely to have observed a single tails if <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> is small and heads is unlikely, which accounts for the decreasing slope. Let's suppose that we now observe two heads, the posterior becomes <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mrow><mi mathvariant=\"normal\">B</mi><mi mathvariant=\"normal\">e</mi><mi mathvariant=\"normal\">t</mi><mi mathvariant=\"normal\">a</mi></mrow><mo stretchy=\"false\">(</mo><mn>3</mn><mo separator=\"true\">,</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathrm{Beta}(3, 2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathrm\">B</span><span class=\"mord mathrm\">e</span><span class=\"mord mathrm\">t</span><span class=\"mord mathrm\">a</span></span><span class=\"mopen\">(</span><span class=\"mord\">3</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">2</span><span class=\"mclose\">)</span></span></span></span> which looks like this.</p>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 583px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/081f6ddc219fb6675288a1b3fcb880fa/6d62d/beta32.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 74.66666666666667%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAAsTAAALEwEAmpwYAAABVklEQVQoz4WT23KDIBCG+/6vluYmV7lIlSgSrcYDBxUJS1exJqY6ZUaHRb799+SH+3cB7H35wEcplWWZ1nqbHQZgFK4EpNyGGWNd122gg4bgApw7rSH8ckq+w1LKOI77vt+QRUCK2ehaS4INOE3TFezzvDH4zmZzOoEkdoL7G3vKE6kUeJ2XggEe0mg5HGEhxAr2Iki26m+1ISLOmGfYTdM8w/ZkkQNL3sk5cvpbBZjhlTL2Jrg4+9hurhDYOe9rhmmSLDBcQ+DN7nj03Vi2V/iGfZ7gcR5Sth4wWDZgrU8bBj3nXFVVGJK+bR2N7JQqTOt1Y631+8cokLgFVlJWdR2HYR5HvTEJpZxzHLsoivCNrrGceO1+v+MUj2Jl2dT1DPvFheh67bMwxqAUukCTUno8HoMgOJ1O5/MZzc/DgRCygvcW/jBlWeZ5jk5xnDCQoihazNG5HwGvamhZcRq/AAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"beta32\" title=\"beta32\" src=\"/static/081f6ddc219fb6675288a1b3fcb880fa/6d62d/beta32.png\" srcset=\"/static/081f6ddc219fb6675288a1b3fcb880fa/a9205/beta32.png 150w,\n/static/081f6ddc219fb6675288a1b3fcb880fa/a8a0d/beta32.png 300w,\n/static/081f6ddc219fb6675288a1b3fcb880fa/6d62d/beta32.png 583w\" sizes=\"(max-width: 583px) 100vw, 583px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n  </a>\n    </span>\n</p>\n<p>Now that we've observed a heads, we can similarly rule out the possibility that <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\theta = 0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span></span></span></span>, so the posterior density there also goes to zero. Furthermore, we've seen more heads than tails, so the posterior density is skewed towards higher values of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>. Let's suppose now that we saw ten tails and twenty heads, so the proportion of heads is the same, but the amount of data goes up. Now the posterior is <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mrow><mi mathvariant=\"normal\">B</mi><mi mathvariant=\"normal\">e</mi><mi mathvariant=\"normal\">t</mi><mi mathvariant=\"normal\">a</mi></mrow><mo stretchy=\"false\">(</mo><mn>21</mn><mo separator=\"true\">,</mo><mn>11</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathrm{Beta}(21, 11)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathrm\">B</span><span class=\"mord mathrm\">e</span><span class=\"mord mathrm\">t</span><span class=\"mord mathrm\">a</span></span><span class=\"mopen\">(</span><span class=\"mord\">2</span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">1</span><span class=\"mord\">1</span><span class=\"mclose\">)</span></span></span></span> which looks like this.</p>\n<p align=\"center\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 553px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/5026119ae12277ba7b5b06594eef67e4/d35da/beta2111.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 78.66666666666667%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAIAAACZeshMAAAACXBIWXMAAAsTAAALEwEAmpwYAAABZUlEQVQoz6VTYW+DIBD1//+xpZ9s0g+dmxW1MlsrWkUDKhw7xa1NR5cluxg8OB7v8XJ45h/hKaWGYXAXAeahrqC52vkjeJomIcQvx8PhTYeBm7ksy7ZtFxpw0HIO9AhZaqRwy3YzW3B5MTWDaw1l8b14A7dLPGVOiBlH4C3yO8BSyr7vnzKnsVET9L0bjJrdYAwpZubF5jnR+vHOTdNwzh2yMTo+W2V1RKFR6m+GWc1ZCm2zLpw+jM3vOLyqqlZmVIWFuw/IAd1awRWDnM7ZnfiZGT37SWuK86rZTtFzPOtBNjLXZWnocYxClRCdkIkcdBzpJJ7GUWk996+USKfO+RDspyTWX+3saa3TNM1INHCeJwklZOw6HI+UKgBKaZ7nuI9m2akotBQXSivGrMEe/gTqXu6Go5CDTexr2e12vu8HQbDdbl/3+zTLXjab9zC8gZ8+CQA8tq5rayq2Q9d1jDHbF1j9BDKwpsHogi8zAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"beta2111\" title=\"beta2111\" src=\"/static/5026119ae12277ba7b5b06594eef67e4/d35da/beta2111.png\" srcset=\"/static/5026119ae12277ba7b5b06594eef67e4/a9205/beta2111.png 150w,\n/static/5026119ae12277ba7b5b06594eef67e4/a8a0d/beta2111.png 300w,\n/static/5026119ae12277ba7b5b06594eef67e4/d35da/beta2111.png 553w\" sizes=\"(max-width: 553px) 100vw, 553px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n  </a>\n    </span>\n</p>\n<p>Now the mode of the posterior distribution is closer to the proportion of heads that we've seen, and the variance has shrunk, i.e. we are more confident of the likely range of values for <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>.</p>\n<p>This illustrates a generic fact about the posterior distribution and its relationship with the prior, namely that as we observe more data, the influence of the prior distribution diminishes, and the posterior distribution starts to become more certain of the values of the parameters in the model.</p>\n<p>The following animation illustrates the coin flipping with two different choices of prior. In this case the data was generated by simulating coin flips with a coin that had probability <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mfrac><mn>3</mn><mn>4</mn></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac 3 4</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.190108em;vertical-align:-0.345em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.845108em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">4</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">3</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.345em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span> of coming up heads. We compare a uniform prior and a <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">B</mi><mi mathvariant=\"normal\">e</mi><mi mathvariant=\"normal\">t</mi><mi mathvariant=\"normal\">a</mi><mo stretchy=\"false\">(</mo><mn>5</mn><mo separator=\"true\">,</mo><mn>11</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathrm{Beta(5,11)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathrm\">B</span><span class=\"mord mathrm\">e</span><span class=\"mord mathrm\">t</span><span class=\"mord mathrm\">a</span><span class=\"mopen\">(</span><span class=\"mord mathrm\">5</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathrm\">1</span><span class=\"mord mathrm\">1</span><span class=\"mclose\">)</span></span></span></span></span> prior that assumes the coin is much more likely to come up tails. To begin with after only a few coin flips the posteriors differ significantly, but as we observe more data they get more and more similar as the influence of the prior fades.</p>\n<p align=\"center\">\n  <img src=\"/56a8cf89da2d9191fcd686805ebf194c/animation.gif\" width=\"900\">\n</p>\n<h2>Inference with the posterior distribution</h2>\n<p>Supposing we've specified the prior and computed the posterior, how do we actually use it? This part turns out to be pretty natural. Most of the time we will want average some quantity of interest over the posterior distribution of parameter values. For example, the posterior mean of the parameters is simply</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"double-struck\">E</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mo>∫</mo><mi>θ</mi><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo stretchy=\"false\">)</mo><mi>d</mi><mi>θ</mi><mi mathvariant=\"normal\">.</mi></mrow><annotation encoding=\"application/x-tex\">  \\mathbb{E}(\\theta | y) = \\int \\theta p(\\theta | y) d\\theta.</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathbb\">E</span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.22225em;vertical-align:-0.86225em;\"></span><span class=\"mop op-symbol large-op\" style=\"margin-right:0.44445em;position:relative;top:-0.0011249999999999316em;\">∫</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">.</span></span></span></span></span>\n<p>Similarly, if we wanted to calculate the probability of some new data <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>~</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\tilde y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8622999999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">~</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span> given observed data <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span></span></span>, we would calculate</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>y</mi><mo>~</mo></mover><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mo>∫</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>y</mi><mo>~</mo></mover><mi mathvariant=\"normal\">∣</mi><mi>θ</mi><mo stretchy=\"false\">)</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo stretchy=\"false\">)</mo><mi>d</mi><mi>θ</mi><mi mathvariant=\"normal\">.</mi></mrow><annotation encoding=\"application/x-tex\">  p(\\tilde y | y) = \\int p(\\tilde y | \\theta) p(\\theta | y) d\\theta.</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">~</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.22225em;vertical-align:-0.86225em;\"></span><span class=\"mop op-symbol large-op\" style=\"margin-right:0.44445em;position:relative;top:-0.0011249999999999316em;\">∫</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">~</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">.</span></span></span></span></span>\n<p>There is a natural interpretation of the right hand side in this context. We calculate the probability (density) of the new data <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>~</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\tilde y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8622999999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">~</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span> for every possible value of the parameters <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span>, the <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>y</mi><mo>~</mo></mover><mi mathvariant=\"normal\">∣</mi><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\tilde y \\vert \\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">~</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span> term; then we compute the weighted average of those where the terms involving more likely parameter values (according to the posterior) contribute more to the average.</p>\n<p>We can also calculate probabilities of parameters taking particular values. Suppose in our coin flipping example we want to know whether <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>θ</mi><mo>&gt;</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><annotation encoding=\"application/x-tex\">\\theta &gt; \\frac 1 2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.73354em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">&gt;</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.190108em;vertical-align:-0.345em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.845108em;\"><span style=\"top:-2.6550000000000002em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.345em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span>. We can just calculate the probability directly by integrating</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mrow><mo fence=\"true\">(</mo><mi>θ</mi><mo>&gt;</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mspace width=\"0.5em\"/><mo fence=\"false\">∣</mo><mspace width=\"0.5em\"/><mi>y</mi><mo fence=\"true\">)</mo></mrow><mo>=</mo><msubsup><mo>∫</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mn>1</mn></msubsup><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo stretchy=\"false\">)</mo><mi>d</mi><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">    P\\left(\\theta &gt; \\frac 1 2 \\hspace{5pt} \\bigg | \\hspace{5pt} y \\right) = \\int_{\\frac 1 2}^1 p(\\theta | y) d\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:2.43em;vertical-align:-0.95003em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">(</span></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">&gt;</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.32144em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\">2</span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\">1</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.5em;\"></span><span class=\"mord\"><span class=\"delimsizing mult\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.4799700000000002em;\"><span style=\"top:-1.65598em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span class=\"delimsizinginner delim-size1\"><span>∣</span></span></span><span style=\"top:-2.25698em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span class=\"delimsizinginner delim-size1\"><span>∣</span></span></span><span style=\"top:-2.85798em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span class=\"delimsizinginner delim-size1\"><span>∣</span></span></span><span style=\"top:-2.87897em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span class=\"delimsizinginner delim-size1\"><span>∣</span></span></span><span style=\"top:-3.47997em;\"><span class=\"pstrut\" style=\"height:2.606em;\"></span><span class=\"delimsizinginner delim-size1\"><span>∣</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9500199999999999em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.5em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">)</span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.7167579999999996em;vertical-align:-1.15275em;\"></span><span class=\"mop\"><span class=\"mop op-symbol large-op\" style=\"margin-right:0.44445em;position:relative;top:-0.0011249999999999316em;\">∫</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5640079999999998em;\"><span style=\"top:-2.08805em;margin-left:-0.44445em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8443142857142858em;\"><span style=\"top:-2.656em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\">2</span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.344em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span></span></span></span><span style=\"top:-4.1129em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.15275em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span></span></span></span></span>\n<p>The right hand side can be expressed in terms of the cumulative distribution function (CDF) of the beta distribution. Since the beta distribution is a commonly used and well understood one-dimensional distribution, numerical implementations of the CDF are readily available. For example, let's suppose we flip the coin thirty times, and observe twenty heads and ten tails. Based on the calculations we did above our posterior distribution is <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mrow><mi mathvariant=\"normal\">B</mi><mi mathvariant=\"normal\">e</mi><mi mathvariant=\"normal\">t</mi><mi mathvariant=\"normal\">a</mi></mrow><mo stretchy=\"false\">(</mo><mn>21</mn><mo separator=\"true\">,</mo><mn>11</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathrm{Beta}(21, 11)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathrm\">B</span><span class=\"mord mathrm\">e</span><span class=\"mord mathrm\">t</span><span class=\"mord mathrm\">a</span></span><span class=\"mopen\">(</span><span class=\"mord\">2</span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">1</span><span class=\"mord\">1</span><span class=\"mclose\">)</span></span></span></span>, so in Python with SciPy we can do</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token keyword\">from</span> scipy<span class=\"token punctuation\">.</span>stats<span class=\"token punctuation\">.</span>distributions <span class=\"token keyword\">import</span> beta\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token number\">1</span> <span class=\"token operator\">-</span> beta<span class=\"token punctuation\">.</span>cdf<span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">21</span><span class=\"token punctuation\">,</span> <span class=\"token number\">11</span><span class=\"token punctuation\">)</span>\n<span class=\"token number\">0.9646222270093858</span></code></pre></div>\n<h2>Inference with posterior samples</h2>\n<p>In general, particularly with high-dimensional posterior distributions where we have many parameters in the model, approximating the integrals we need to evaluate to do inference will be much harder. It turns out though, that we can get good approximations to these integrals if we have access to samples from the posterior distribution. For example, this code snippet draws ten thousands samples from the posterior, then calculates the proportion that are greater than <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">0.5</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">5</span></span></span></span>. The answer is a close approximation to the answer obtained from the CDF above.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> samples <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>beta<span class=\"token punctuation\">(</span><span class=\"token number\">21</span><span class=\"token punctuation\">,</span> <span class=\"token number\">11</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10000</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token punctuation\">(</span>samples <span class=\"token operator\">></span> <span class=\"token number\">0.5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token number\">0.9653</span></code></pre></div>\n<p>This is a specific example of using a <a href=\"https://en.wikipedia.org/wiki/Monte_Carlo_integration\">Monte Carlo estimate</a> to approximate an integral. In general, if we have an integral of the form</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>∫</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mo stretchy=\"false\">)</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo stretchy=\"false\">)</mo><mi>d</mi><mi>θ</mi><mo separator=\"true\">,</mo></mrow><annotation encoding=\"application/x-tex\">  \\int f(\\theta) p(\\theta | y) d\\theta,</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:2.22225em;vertical-align:-0.86225em;\"></span><span class=\"mop op-symbol large-op\" style=\"margin-right:0.44445em;position:relative;top:-0.0011249999999999316em;\">∫</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mpunct\">,</span></span></span></span></span>\n<p>we can approximate that with a sum of the form</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mfrac><mn>1</mn><mi>S</mi></mfrac><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mi>f</mi><mo stretchy=\"false\">(</mo><msup><mi>θ</mi><mi>s</mi></msup><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo></mrow><annotation encoding=\"application/x-tex\">  \\frac 1 S \\sum_{s=1}^S f(\\theta^s),</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:3.0954490000000003em;vertical-align:-1.267113em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.32144em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\">1</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.8283360000000002em;\"><span style=\"top:-1.882887em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">s</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3000050000000005em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05764em;\">S</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.267113em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7143919999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">s</span></span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span></span></span></span></span>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>θ</mi><mi>s</mi></msup><mspace width=\"0.5em\"/><mi>s</mi><mo>=</mo><mn>1</mn><mo separator=\"true\">,</mo><mo>…</mo><mo separator=\"true\">,</mo><mi>S</mi></mrow><annotation encoding=\"application/x-tex\">\\theta^s \\hspace{5pt} s = 1, \\dots, S</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">s</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.5em;\"></span><span class=\"mord mathdefault\">s</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\">…</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span></span></span></span> are samples from the distribution defined by <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\theta \\vert y)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose\">)</span></span></span></span>. That is, we draw posterior samples then evaluate the integrand on those samples and average. If you are familiar with numerical integration methods, you can think of this like a grid approximation with a dynamic grid, where the grid density is higher in regions where the probability density is higher.</p>\n<p>Given a set of samples <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>θ</mi><mi>s</mi></msup><mspace width=\"0.5em\"/><mi>s</mi><mo>=</mo><mn>1</mn><mo separator=\"true\">,</mo><mo>…</mo><mo separator=\"true\">,</mo><mi>S</mi></mrow><annotation encoding=\"application/x-tex\">\\theta^s \\hspace{5pt} s = 1, \\dots, S</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">s</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.5em;\"></span><span class=\"mord mathdefault\">s</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\">…</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span></span></span></span> and using the integral formulas above, we can approximate posterior means</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mfrac><mn>1</mn><mi>S</mi></mfrac><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><msup><mi>θ</mi><mi>s</mi></msup><mo separator=\"true\">,</mo></mrow><annotation encoding=\"application/x-tex\">  \\frac 1 S \\sum_{s=1}^S \\theta^s,</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:3.0954490000000003em;vertical-align:-1.267113em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.32144em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\">1</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.8283360000000002em;\"><span style=\"top:-1.882887em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">s</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3000050000000005em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05764em;\">S</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.267113em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7143919999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">s</span></span></span></span></span></span></span></span><span class=\"mpunct\">,</span></span></span></span></span>\n<p>and the posterior predictive distribution (the probability of observing new data <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mover accent=\"true\"><mi>y</mi><mo>~</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\tilde y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8622999999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">~</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span>)</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mfrac><mn>1</mn><mi>S</mi></mfrac><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mi>p</mi><mo stretchy=\"false\">(</mo><mover accent=\"true\"><mi>y</mi><mo>~</mo></mover><mi mathvariant=\"normal\">∣</mi><msup><mi>θ</mi><mi>s</mi></msup><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">.</mi></mrow><annotation encoding=\"application/x-tex\">  \\frac 1 S \\sum_{s=1}^S p(\\tilde y | \\theta^s).</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:3.0954490000000003em;vertical-align:-1.267113em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.32144em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\">1</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.8283360000000002em;\"><span style=\"top:-1.882887em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">s</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span><span style=\"top:-4.3000050000000005em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05764em;\">S</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.267113em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6678599999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">y</span></span><span style=\"top:-3.35em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.19444em;\"><span class=\"mord\">~</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7143919999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">s</span></span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mord\">.</span></span></span></span></span>\n<p>In our case, with the samples we drew before, our approximation of the posterior mean is</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> samples<span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token number\">0.6562683695235446</span></code></pre></div>\n<p>Similarly, if we wanted to predict the probability of getting a heads, then tails (in that order) on the next two coin flips, we have <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mtext>heads then tails</mtext><mi mathvariant=\"normal\">∣</mi><mi>θ</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>θ</mi><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p(\\text{heads then tails} \\vert \\theta) = \\theta (1 - \\theta)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">heads then tails</span></span><span class=\"mord\">∣</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mopen\">(</span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span></span></span></span>, which we can approximate with the samples as follows</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token punctuation\">(</span>samples <span class=\"token operator\">*</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span> <span class=\"token operator\">-</span> samples<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token number\">0.2187</span></code></pre></div>\n<p>Just about any inference we might like to make about the parameters in our model can be calculated in a natural way like this, provided you can draw samples from the posterior distribution. Though it is out of scope for this particular series of blog posts, a popular family of algorithms for drawing samples from arbitrary probability distirbutions are Markov chain Monte Carlo (MCMC) algorithms. One of the nice features of these algorithms is that they often don't require you to normalise the posterior distribution, it suffices to know it up to a multiplicative constant. In the context of Bayesian statistics where we calculate the posterior as the product of two terms that ordinarily would then need to be normalised, this is extremely convenient! If you are interested in seeing how we could have used an MCMC sampler to sample from our posterior, check out the addendum about drawing posterior samples using Stan.</p>\n<p>In the next post we'll describe hierarchical models of which multi-level regression is an example.</p>\n<h2>Addendum: drawing posterior samples using Stan</h2>\n<p>In the coin flipping example above, we knew the precise form of the posterior, and it happened to be something that could easily be sampled from using functions built into widely used libraries like NumPy. If we had a more complicated posterior we would probably need to turn to specialised sampling tools. In this addendum we'll take a quick look at how we can draw samples from the posterior using Stan and PyStan. We'll keep using the coin flipping example so we can compare to earlier results, but everything we will do generalises to more complicated models and distributions.</p>\n<p>Stan implements a powerful sampling algorithm called Hamiltonian Monte Carlo (HMC). The details of how it works are beyond the scope of this post, but there is one important thing we should know in order to understand the output of Stan. HMC, and MCMC algorithms more generally, draw correlated, not fully independent samples. Provided we properly account for the correlation in the samples though, this isn't a barrier to making inferences.</p>\n<p>Stan has its own modelling language which we use to specify the model. The Stan compiler <code class=\"language-text\">stanc</code> will transpile the Stan modelling language to optimized C++ code, then compile the C++ code for us. The result is a very efficient sampling algorithm for the posterior distribution of our model. We won't discuss the syntax or features of the Stan modelling language here, check out the very good <a href=\"https://mc-stan.org/users/documentation/\">documentation</a> if you would like to know more.</p>\n<p>We need to define the data, parameters and then specify the prior and sampling distribution. We do so in three blocks of code labelled <code class=\"language-text\">data</code>, <code class=\"language-text\">parameters</code> and <code class=\"language-text\">model</code>. Stan does the rest by computing the posterior and drawing samples using HMC. Here is the full model we are going to use.</p>\n<div class=\"gatsby-highlight\" data-language=\"stan\"><pre class=\"language-stan\"><code class=\"language-stan\"><span class=\"token keyword\">data</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token keyword\">int</span><span class=\"token constraint\"><span class=\"token punctuation\">&lt;</span><span class=\"token property\">lower</span><span class=\"token operator\">=</span><span class=\"token expression\"><span class=\"token number\">0</span></span><span class=\"token punctuation\">></span></span> N<span class=\"token punctuation\">;</span>  <span class=\"token comment\">// number of coin flips</span>\n  <span class=\"token keyword\">int</span><span class=\"token constraint\"><span class=\"token punctuation\">&lt;</span><span class=\"token property\">lower</span><span class=\"token operator\">=</span><span class=\"token expression\"><span class=\"token number\">0</span></span><span class=\"token punctuation\">,</span> <span class=\"token property\">upper</span><span class=\"token operator\">=</span><span class=\"token expression\">N</span><span class=\"token punctuation\">></span></span> y<span class=\"token punctuation\">;</span>  <span class=\"token comment\">// number of heads</span>\n<span class=\"token punctuation\">}</span>\n<span class=\"token keyword\">parameters</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token keyword\">real</span><span class=\"token constraint\"><span class=\"token punctuation\">&lt;</span><span class=\"token property\">lower</span><span class=\"token operator\">=</span><span class=\"token expression\"><span class=\"token number\">0</span></span><span class=\"token punctuation\">,</span> <span class=\"token property\">upper</span><span class=\"token operator\">=</span><span class=\"token expression\"><span class=\"token number\">1</span></span><span class=\"token punctuation\">></span></span> theta<span class=\"token punctuation\">;</span>  <span class=\"token comment\">// probability of heads</span>\n<span class=\"token punctuation\">}</span>\n<span class=\"token keyword\">model</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token comment\">// the prior</span>\n  theta <span class=\"token operator\">~</span> <span class=\"token function\">uniform</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  <span class=\"token comment\">// the sampling distribution</span>\n  y <span class=\"token operator\">~</span> <span class=\"token function\">binomial</span><span class=\"token punctuation\">(</span>N<span class=\"token punctuation\">,</span> theta<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>Let's look at each block in turn. First the <code class=\"language-text\">data</code> block.</p>\n<div class=\"gatsby-highlight\" data-language=\"stan\"><pre class=\"language-stan\"><code class=\"language-stan\"><span class=\"token keyword\">data</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token keyword\">int</span><span class=\"token constraint\"><span class=\"token punctuation\">&lt;</span><span class=\"token property\">lower</span><span class=\"token operator\">=</span><span class=\"token expression\"><span class=\"token number\">0</span></span><span class=\"token punctuation\">></span></span> N<span class=\"token punctuation\">;</span>  <span class=\"token comment\">// number of coin flips</span>\n  <span class=\"token keyword\">int</span><span class=\"token constraint\"><span class=\"token punctuation\">&lt;</span><span class=\"token property\">lower</span><span class=\"token operator\">=</span><span class=\"token expression\"><span class=\"token number\">0</span></span><span class=\"token punctuation\">,</span> <span class=\"token property\">upper</span><span class=\"token operator\">=</span><span class=\"token expression\">N</span><span class=\"token punctuation\">></span></span> y<span class=\"token punctuation\">;</span>  <span class=\"token comment\">// number of heads</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>We specify that Stan should expect two pieces of information: the number of times we flipped the coin <code class=\"language-text\">N</code>, and the number of times it came up heads <code class=\"language-text\">y</code>. Stan is a typed language, so we specify that both <code class=\"language-text\">N</code> and <code class=\"language-text\">y</code> are integers. Additionally, Stan allows us to place constraints on the data which will be checked at runtime. We specify that we can't have a negative number of coin flips, and the number of heads should be non-negative and less than <code class=\"language-text\">N</code>.</p>\n<p>Next the parameters block</p>\n<div class=\"gatsby-highlight\" data-language=\"stan\"><pre class=\"language-stan\"><code class=\"language-stan\"><span class=\"token keyword\">parameters</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token keyword\">real</span><span class=\"token constraint\"><span class=\"token punctuation\">&lt;</span><span class=\"token property\">lower</span><span class=\"token operator\">=</span><span class=\"token expression\"><span class=\"token number\">0</span></span><span class=\"token punctuation\">,</span> <span class=\"token property\">upper</span><span class=\"token operator\">=</span><span class=\"token expression\"><span class=\"token number\">1</span></span><span class=\"token punctuation\">></span></span> theta<span class=\"token punctuation\">;</span>  <span class=\"token comment\">// probability of heads</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>We have only one parameter in our model, <code class=\"language-text\">theta</code>. This is the probability that a single flip will come up heads. As such it should be a real number between 0 and 1.</p>\n<p>Finally the model</p>\n<div class=\"gatsby-highlight\" data-language=\"stan\"><pre class=\"language-stan\"><code class=\"language-stan\"><span class=\"token keyword\">model</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token comment\">// the prior</span>\n  theta <span class=\"token operator\">~</span> <span class=\"token function\">uniform</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  <span class=\"token comment\">// the sampling distribution</span>\n  y <span class=\"token operator\">~</span> <span class=\"token function\">binomial</span><span class=\"token punctuation\">(</span>N<span class=\"token punctuation\">,</span> theta<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>We specify a uniform prior on theta (which is actually redundant, if we hadn't specified a prior on <code class=\"language-text\">theta</code> then Stan would have automatically chosen a uniform prior for us), and a binomial sampling distribution for <code class=\"language-text\">y</code>. The Stan syntax mirrors mathematical notation quite nicely, so the model specification is quite readable.</p>\n<p>To compile the model and draw samples we're going to use the Python interface PyStan. There are however many alternatives including interfaces for R, Matlab, Julia and the command line. Details can be found in the <a href=\"https://mc-stan.org/users/documentation/\">Stan documentation</a>. To get started with PyStan, install it with <code class=\"language-text\">pip</code>.</p>\n<div class=\"gatsby-highlight\" data-language=\"sh\"><pre class=\"language-sh\"><code class=\"language-sh\">pip install pystan</code></pre></div>\n<p>Now we can load the model as a <code class=\"language-text\">pystan.StanModel</code> object and use the <code class=\"language-text\">sampling</code> method to draw samples. Data is passed to the model using a Python dictionary as follows. We'll assume twenty heads and ten tails like we did previously.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> pystan\n\nsm <span class=\"token operator\">=</span> pystan<span class=\"token punctuation\">.</span>StanModel<span class=\"token punctuation\">(</span><span class=\"token builtin\">file</span><span class=\"token operator\">=</span><span class=\"token string\">\"model.stan\"</span><span class=\"token punctuation\">)</span>\nfit <span class=\"token operator\">=</span> sm<span class=\"token punctuation\">.</span>sampling<span class=\"token punctuation\">(</span>data<span class=\"token operator\">=</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"N\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">30</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"y\"</span><span class=\"token punctuation\">:</span> <span class=\"token number\">20</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">iter</span><span class=\"token operator\">=</span><span class=\"token number\">5000</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>If you run the above code snippet, Stan will create four chains (the default), and draw 5000 samples from each. MCMC samplers can be sensitive to initial conditions, so running multiple independent chains helps us assess whether the chains have converged to the correct distributions. For each chain we discard the first 50% of samples as warmup, leaving us with a total of 10000 post-warmup draws from the four chains. Stan prints a summary of the sampling like the one below.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Inference for Stan model: anon_model_bd7785d44829c52cfe28d5321faadbc2.\n4 chains, each with iter=5000; warmup=2500; thin=1;\npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n        mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\ntheta   0.66  1.3e-3   0.08   0.49    0.6   0.66   0.71   0.81   3822    1.0\nlp__  -21.09    0.01   0.69 -23.09 -21.25 -20.82 -20.64 -20.59   3845    1.0\n\nSamples were drawn using NUTS at Sun Nov 10 10:12:21 2019.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at\nconvergence, Rhat=1).</code></pre></div>\n<p>For each parameter in our model, in this case <code class=\"language-text\">theta</code>, we get an estimate of the posterior mean, the standard error, the standard deviation and various percentiles. This means at a glance we can read off some basic information about <code class=\"language-text\">theta</code> such as the posterior mean, which in this case is <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0.66</mn></mrow><annotation encoding=\"application/x-tex\">0.66</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">6</span><span class=\"mord\">6</span></span></span></span>. We can also see that a central 95% credible interval for <code class=\"language-text\">theta</code> would be <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0.49</mn></mrow><annotation encoding=\"application/x-tex\">0.49</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">4</span><span class=\"mord\">9</span></span></span></span> to <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0.81</mn></mrow><annotation encoding=\"application/x-tex\">0.81</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">8</span><span class=\"mord\">1</span></span></span></span>, so there's still a fairly wide range of credible values.</p>\n<p>The final two columns <code class=\"language-text\">n_eff</code> and <code class=\"language-text\">Rhat</code> help us understand how correlated the samples are, and how well the sampler converged. <code class=\"language-text\">n_eff</code> counts the \"effective\" number of samples, roughly speaking we can expect the correlated samples drawn by Stan to have the utility of this number of independent samples. <code class=\"language-text\">Rhat</code> measures how well the chains have converged and whether they have converged to the same thing. An <code class=\"language-text\">Rhat</code> value of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">.</span><span class=\"mord\">0</span></span></span></span> is necessary but not sufficient for convergence, i.e. if the chains have converged then <code class=\"language-text\">Rhat</code> will be <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">.</span><span class=\"mord\">0</span></span></span></span>, though if <code class=\"language-text\">Rhat</code> is <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">.</span><span class=\"mord\">0</span></span></span></span> convergence isn't guaranteed. In our case we have a large number of effective samples and an <code class=\"language-text\">Rhat</code> of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">.</span><span class=\"mord\">0</span></span></span></span>, so sampling appears to have been successful.</p>\n<p>We can get the sampled values of <code class=\"language-text\">theta</code> and do computations with them manually, for example, let's calculate the probability that <code class=\"language-text\">theta</code> is greater than <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">0.5</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">5</span></span></span></span> and compare to our earlier answers</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> samples <span class=\"token operator\">=</span> fit<span class=\"token punctuation\">.</span>extract<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"theta\"</span><span class=\"token punctuation\">]</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token punctuation\">(</span>samples <span class=\"token operator\">></span> <span class=\"token number\">0.5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token number\">0.9685</span></code></pre></div>\n<p>or the probability we next flip a heads, then tails</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token punctuation\">(</span>samples <span class=\"token operator\">*</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span> <span class=\"token operator\">-</span> samples<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token number\">0.21900253298714248</span></code></pre></div>\n<p>Both answers are in line with the results we saw earlier, but this time around we didn't have to explicitly compute the posterior distribution! Stan is able to do all of the heavy lifting for us. When dealing with more complicated distributions than what we've been considering, this turns out to be crucial for making inference tractable.</p>","excerpt":"This is the second post in a series on election modelling; specifically multi-level regression with poststratification (MRP) and its…"}},"pageContext":{"next":{"frontmatter":{"path":"/blog/election-modelling-part-3","title":"Election Modelling - Part 3","tags":["bayesian statistics","politics"]},"fields":{"collection":"posts"},"fileAbsolutePath":"/Users/tombegley/www/tcbegley.com/src/posts/election-modelling-3.md"},"previous":{"frontmatter":{"path":"/blog/election-modelling-part-1","title":"Election Modelling - Part 1","tags":["bayesian statistics","politics"]},"fields":{"collection":"posts"},"fileAbsolutePath":"/Users/tombegley/www/tcbegley.com/src/posts/election-modelling-1.md"}}},"staticQueryHashes":["1425477374","3128451518"]}